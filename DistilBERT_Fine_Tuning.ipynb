{"cells":[{"cell_type":"markdown","source":["# Many thanks to:\n","\n","\"laxmimerit/NLP-Tutorials-with-HuggingFace\n","\n","AVP, Data Scientist @ IGP.COM | Lead Data Scientist @ KGPTalkie 50K+ YouTube Subscribers | Know more https://kgptalkie.com\"\n","\n","https://github.com/laxmimerit/NLP-Tutorials-with-HuggingFace"],"metadata":{"id":"yISGU3kYkuP_"}},{"cell_type":"markdown","source":["# **Fine-Tuning DistilBERT for Multi-Label Text Classification**\n","# (**Auto Tagging of Reaserch papers abstracts**)"],"metadata":{"id":"YPuiX3RoVpwJ"}},{"cell_type":"code","source":["from IPython.display import HTML\n","shell = get_ipython()\n","\n","def adjust_font_size():\n","  display(HTML('''<style>\n","    body {\n","      font-size: 20px;\n","    }\n","  '''))\n","\n","if adjust_font_size not in shell.events.callbacks['pre_execute']:\n","  shell.events.register('pre_execute', adjust_font_size)"],"metadata":{"id":"52V8a_lN0auF","executionInfo":{"status":"ok","timestamp":1715081440222,"user_tz":-180,"elapsed":907,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Installation"],"metadata":{"id":"WJ07ZQlqyzfu"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"qZR93i79-_sp","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"cc9e9c76-ab85-4eed-b68c-820d9a2b878e","executionInfo":{"status":"ok","timestamp":1715081562986,"user_tz":-180,"elapsed":122777,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.30.0-py3-none-any.whl (302 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/302.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/302.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.30.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n","Collecting transformers\n","  Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.40.1\n","    Uninstalling transformers-4.40.1:\n","      Successfully uninstalled transformers-4.40.1\n","Successfully installed transformers-4.40.2\n","Collecting datasets\n","  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Collecting huggingface-hub>=0.21.2 (from datasets)\n","  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.20.3\n","    Uninstalling huggingface-hub-0.20.3:\n","      Successfully uninstalled huggingface-hub-0.20.3\n","Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n"]}],"source":["# install required packages\n","!pip install -U accelerate\n","!pip install -U transformers\n","!pip install datasets"]},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"k-N3y1Ghy5sl"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"GxTHGeuDARPH","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"4b0fa406-94c9-43a2-c800-478ab1bda06f","executionInfo":{"status":"ok","timestamp":1715081574375,"user_tz":-180,"elapsed":11404,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["# required libraries for Data Loading and Data pre-Processing\n","from datasets import load_dataset\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import nltk\n","import re\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","# required libraries for Data visulization\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","%matplotlib inline\n","\n","# required libararies for model building\n","from transformers import DistilBertTokenizer, AutoTokenizer\n","\n","# DistilBertForSequenceClassification insted of ditilbertbaseduncased because it will automaticaly add a classification head on top of this masked language model but ditilbertbaseduncased work only for masked language modleing\n","from transformers import DistilBertForSequenceClassification, AutoModelForSequenceClassification #AutoModelForSequenceClassification also to provide classification head\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset\n","\n","# required libraries for Model Evaluatuion\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, hamming_loss\n","from transformers import EvalPrediction\n","\n","# to save important object such as the multilabelbinarizaer object as a file\n","import pickle"]},{"cell_type":"markdown","source":["# **Data**"],"metadata":{"id":"mnofYzo8Q6W9"}},{"cell_type":"markdown","source":["## Data Loading"],"metadata":{"id":"rpPSbO93Q3ue"}},{"cell_type":"code","source":["dataset = load_dataset(\"jamescalam/ai-arxiv\", split='train')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339,"referenced_widgets":["0ee05d50dffb42958bc99478048823b4","6d2516ddea45451da9b69715f2ab5730","58d7a98266684f48bec53363b8b0e247","38f84c4b82cd47328ee449b4af501996","d32167037f9b4509b210d66debe35509","29671998bfab4c96bd1eede08b441e21","679f3918a8a3412b8caebbaef89d0889","8bd065f5e7524f24925c8f1303f40cd7","313e28c96f5043bb9c310fcb98ce737e","9c8f5f8deb854017ada8efaa2dca6acc","3dfea1bfcfe64b7a869350089ab4c2d4","f2a7710208b94b2d9e9b6ad5417a089b","09adc0e6e7d9480d89eaa4d4cd356e7f","622c3ac2ae4f4ef1b8ebed42c5b6690a","9bbb985f0c8f4b47900a9d987777606e","a330bbfbe3924eeb97e923c7022a68d5","b150f2ac63bd48b0b650a0a8b26930c6","976349b877fc4a188886148d8b63451b","205d16bef264488b9c737e35159d7181","5bbfb18c211e4a4da59dbdd01dd5af99","459b47e7fdd449d4a928296c411d97d4","0e8938d8f1b04aaab0fff3665adea805","998c83e669f0424d8ff8e266078ee818","8fcb10171be343e592e1f93501cce45f","6e939acb564341be88eff10764481688","42d015f6fa10487aa90ca29b49bac292","01d6f917bee344309cad8c1dbde932e9","a071a74c6c954d8bbc1c4f0c7c5906f0","c535c4b31d424d678d625284187bd416","3fc0feb049c04d99b32e5e4d791443af","c27d009d39cd423ea6f876fd239712ec","ef17f98f8c2c4e88a5fbc78587e946d8","e2e03eb9ecab42d59a8e52b747c42f45"]},"id":"77AxTeVBEL-P","outputId":"7fd78b24-3f1b-491f-b322-e701b18de3ed","executionInfo":{"status":"ok","timestamp":1715081581614,"user_tz":-180,"elapsed":6842,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/267 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ee05d50dffb42958bc99478048823b4"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Repo card metadata block was not found. Setting CardData to empty.\n","WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/38.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2a7710208b94b2d9e9b6ad5417a089b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/423 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"998c83e669f0424d8ff8e266078ee818"}},"metadata":{}}]},{"cell_type":"markdown","source":["## Data Exploration & Pre-Processing"],"metadata":{"id":"NrFewMh0OCc0"}},{"cell_type":"code","source":["# coverting dataset into a DataFrame for easier manipulation\n","dataset = pd.DataFrame(dataset)\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"11QzoCQgzJQ7","outputId":"88597c38-9edb-4594-ee65-f214ab834552","executionInfo":{"status":"ok","timestamp":1715081582807,"user_tz":-180,"elapsed":1201,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["             id                                              title  \\\n","0    2210.03945      Understanding HTML with Large Language Models   \n","1    1711.05101              Decoupled Weight Decay Regularization   \n","2    2305.17493  The Curse of Recursion: Training on Generated ...   \n","3    2205.09712  Selection-Inference: Exploiting Large Language...   \n","4    2104.06001                 Gender Bias in Machine Translation   \n","..          ...                                                ...   \n","418  2204.06745  GPT-NeoX-20B: An Open-Source Autoregressive La...   \n","419  1707.06347            Proximal Policy Optimization Algorithms   \n","420  2307.09288  Llama 2: Open Foundation and Fine-Tuned Chat M...   \n","421  2211.05100  BLOOM: A 176B-Parameter Open-Access Multilingu...   \n","422  2009.10031  Training Production Language Models without Me...   \n","\n","                                               summary  \\\n","0    Large language models (LLMs) have shown except...   \n","1    L$_2$ regularization and weight decay regulari...   \n","2    Stable Diffusion revolutionised image creation...   \n","3    Large language models (LLMs) have been shown t...   \n","4    Machine translation (MT) technology has facili...   \n","..                                                 ...   \n","418  We introduce GPT-NeoX-20B, a 20 billion parame...   \n","419  We propose a new family of policy gradient met...   \n","420  In this work, we develop and release Llama 2, ...   \n","421  Large language models (LLMs) have been shown t...   \n","422  This paper presents the first consumer-scale n...   \n","\n","                              source  \\\n","0    http://arxiv.org/pdf/2210.03945   \n","1    http://arxiv.org/pdf/1711.05101   \n","2    http://arxiv.org/pdf/2305.17493   \n","3    http://arxiv.org/pdf/2205.09712   \n","4    http://arxiv.org/pdf/2104.06001   \n","..                               ...   \n","418  http://arxiv.org/pdf/2204.06745   \n","419  http://arxiv.org/pdf/1707.06347   \n","420  http://arxiv.org/pdf/2307.09288   \n","421  http://arxiv.org/pdf/2211.05100   \n","422  http://arxiv.org/pdf/2009.10031   \n","\n","                                               authors  \\\n","0    [Izzeddin Gur, Ofir Nachum, Yingjie Miao, Must...   \n","1                      [Ilya Loshchilov, Frank Hutter]   \n","2    [Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao,...   \n","3    [Antonia Creswell, Murray Shanahan, Irina Higg...   \n","4    [Beatrice Savoldi, Marco Gaido, Luisa Bentivog...   \n","..                                                 ...   \n","418  [Sid Black, Stella Biderman, Eric Hallahan, Qu...   \n","419  [John Schulman, Filip Wolski, Prafulla Dhariwa...   \n","420  [Hugo Touvron, Louis Martin, Kevin Stone, Pete...   \n","421  [BigScience Workshop, :, Teven Le Scao, Angela...   \n","422  [Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews,...   \n","\n","                              categories  \\\n","0                         [cs.LG, cs.AI]   \n","1                [cs.LG, cs.NE, math.OC]   \n","2    [cs.LG, cs.AI, cs.CL, cs.CR, cs.CV]   \n","3                         [cs.AI, cs.CL]   \n","4                                [cs.CL]   \n","..                                   ...   \n","418                              [cs.CL]   \n","419                              [cs.LG]   \n","420                       [cs.CL, cs.AI]   \n","421                              [cs.CL]   \n","422              [cs.LG, cs.CR, stat.ML]   \n","\n","                                               comment journal_ref  \\\n","0                                                 None        None   \n","1         Published as a conference paper at ICLR 2019        None   \n","2                                                 None        None   \n","3                                                 None        None   \n","4    Accepted for publication in Transaction of the...        None   \n","..                                                 ...         ...   \n","418  To appear in the Proceedings of the ACL Worksh...        None   \n","419                                               None        None   \n","420                                               None        None   \n","421                                               None        None   \n","422                                               None        None   \n","\n","    primary_category published   updated  \\\n","0              cs.LG  20221008  20230519   \n","1              cs.LG  20171114  20190104   \n","2              cs.LG  20230527  20230531   \n","3              cs.AI  20220519  20220519   \n","4              cs.CL  20210413  20210507   \n","..               ...       ...       ...   \n","418            cs.CL  20220414  20220414   \n","419            cs.LG  20170720  20170828   \n","420            cs.CL  20230718  20230719   \n","421            cs.CL  20221109  20230627   \n","422            cs.LG  20200921  20200921   \n","\n","                                               content  \\\n","0    UNDERSTANDING HTML WITH LARGE LANGUAGE\\nMODELS...   \n","1    Published as a conference paper at ICLR 2019\\n...   \n","2    THECURSE OF RECURSION :\\nTRAINING ON GENERATED...   \n","3    2022-5-20\\nSelection-Inference: Exploiting Lar...   \n","4    Gender Bias in Machine Translation\\nBeatrice S...   \n","..                                                 ...   \n","418  GPT-NeoX-20B: An Open-Source Autoregressive La...   \n","419  Proximal Policy Optimization Algorithms\\nJohn ...   \n","420  L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle : Open ...   \n","421  BLOOM: A 176B-Parameter Open-Access Multilingu...   \n","422  1\\nTraining Production Language Models\\nwithou...   \n","\n","                                            references  \n","0    [{'id': '2107.06955'}, {'id': '2204.02311'}, {...  \n","1    [{'id': '1707.08819'}, {'id': '1705.08292'}, {...  \n","2    [{'id': '1906.02243'}, {'id': '1810.04805'}, {...  \n","3    [{'id': '2205.09712'}, {'id': '2112.03753'}, {...  \n","4    [{'id': '2104.06001'}, {'id': '2012.15859'}, {...  \n","..                                                 ...  \n","418  [{'id': '1910.05895'}, {'id': '2110.06609'}, {...  \n","419  [{'id': '1604.06778'}, {'id': '1506.02438'}, {...  \n","420  [{'id': '2302.13971'}, {'id': '1908.01091'}, {...  \n","421  [{'id': '2208.07339'}, {'id': '2204.02311'}, {...  \n","422  [{'id': '2009.10031'}, {'id': '1610.05755'}, {...  \n","\n","[423 rows x 13 columns]"],"text/html":["\n","  <div id=\"df-e1193d7d-d317-4ce9-8762-a594f3b3fe60\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>title</th>\n","      <th>summary</th>\n","      <th>source</th>\n","      <th>authors</th>\n","      <th>categories</th>\n","      <th>comment</th>\n","      <th>journal_ref</th>\n","      <th>primary_category</th>\n","      <th>published</th>\n","      <th>updated</th>\n","      <th>content</th>\n","      <th>references</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2210.03945</td>\n","      <td>Understanding HTML with Large Language Models</td>\n","      <td>Large language models (LLMs) have shown except...</td>\n","      <td>http://arxiv.org/pdf/2210.03945</td>\n","      <td>[Izzeddin Gur, Ofir Nachum, Yingjie Miao, Must...</td>\n","      <td>[cs.LG, cs.AI]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>cs.LG</td>\n","      <td>20221008</td>\n","      <td>20230519</td>\n","      <td>UNDERSTANDING HTML WITH LARGE LANGUAGE\\nMODELS...</td>\n","      <td>[{'id': '2107.06955'}, {'id': '2204.02311'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1711.05101</td>\n","      <td>Decoupled Weight Decay Regularization</td>\n","      <td>L$_2$ regularization and weight decay regulari...</td>\n","      <td>http://arxiv.org/pdf/1711.05101</td>\n","      <td>[Ilya Loshchilov, Frank Hutter]</td>\n","      <td>[cs.LG, cs.NE, math.OC]</td>\n","      <td>Published as a conference paper at ICLR 2019</td>\n","      <td>None</td>\n","      <td>cs.LG</td>\n","      <td>20171114</td>\n","      <td>20190104</td>\n","      <td>Published as a conference paper at ICLR 2019\\n...</td>\n","      <td>[{'id': '1707.08819'}, {'id': '1705.08292'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2305.17493</td>\n","      <td>The Curse of Recursion: Training on Generated ...</td>\n","      <td>Stable Diffusion revolutionised image creation...</td>\n","      <td>http://arxiv.org/pdf/2305.17493</td>\n","      <td>[Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao,...</td>\n","      <td>[cs.LG, cs.AI, cs.CL, cs.CR, cs.CV]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>cs.LG</td>\n","      <td>20230527</td>\n","      <td>20230531</td>\n","      <td>THECURSE OF RECURSION :\\nTRAINING ON GENERATED...</td>\n","      <td>[{'id': '1906.02243'}, {'id': '1810.04805'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2205.09712</td>\n","      <td>Selection-Inference: Exploiting Large Language...</td>\n","      <td>Large language models (LLMs) have been shown t...</td>\n","      <td>http://arxiv.org/pdf/2205.09712</td>\n","      <td>[Antonia Creswell, Murray Shanahan, Irina Higg...</td>\n","      <td>[cs.AI, cs.CL]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>cs.AI</td>\n","      <td>20220519</td>\n","      <td>20220519</td>\n","      <td>2022-5-20\\nSelection-Inference: Exploiting Lar...</td>\n","      <td>[{'id': '2205.09712'}, {'id': '2112.03753'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2104.06001</td>\n","      <td>Gender Bias in Machine Translation</td>\n","      <td>Machine translation (MT) technology has facili...</td>\n","      <td>http://arxiv.org/pdf/2104.06001</td>\n","      <td>[Beatrice Savoldi, Marco Gaido, Luisa Bentivog...</td>\n","      <td>[cs.CL]</td>\n","      <td>Accepted for publication in Transaction of the...</td>\n","      <td>None</td>\n","      <td>cs.CL</td>\n","      <td>20210413</td>\n","      <td>20210507</td>\n","      <td>Gender Bias in Machine Translation\\nBeatrice S...</td>\n","      <td>[{'id': '2104.06001'}, {'id': '2012.15859'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>418</th>\n","      <td>2204.06745</td>\n","      <td>GPT-NeoX-20B: An Open-Source Autoregressive La...</td>\n","      <td>We introduce GPT-NeoX-20B, a 20 billion parame...</td>\n","      <td>http://arxiv.org/pdf/2204.06745</td>\n","      <td>[Sid Black, Stella Biderman, Eric Hallahan, Qu...</td>\n","      <td>[cs.CL]</td>\n","      <td>To appear in the Proceedings of the ACL Worksh...</td>\n","      <td>None</td>\n","      <td>cs.CL</td>\n","      <td>20220414</td>\n","      <td>20220414</td>\n","      <td>GPT-NeoX-20B: An Open-Source Autoregressive La...</td>\n","      <td>[{'id': '1910.05895'}, {'id': '2110.06609'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>419</th>\n","      <td>1707.06347</td>\n","      <td>Proximal Policy Optimization Algorithms</td>\n","      <td>We propose a new family of policy gradient met...</td>\n","      <td>http://arxiv.org/pdf/1707.06347</td>\n","      <td>[John Schulman, Filip Wolski, Prafulla Dhariwa...</td>\n","      <td>[cs.LG]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>cs.LG</td>\n","      <td>20170720</td>\n","      <td>20170828</td>\n","      <td>Proximal Policy Optimization Algorithms\\nJohn ...</td>\n","      <td>[{'id': '1604.06778'}, {'id': '1506.02438'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>420</th>\n","      <td>2307.09288</td>\n","      <td>Llama 2: Open Foundation and Fine-Tuned Chat M...</td>\n","      <td>In this work, we develop and release Llama 2, ...</td>\n","      <td>http://arxiv.org/pdf/2307.09288</td>\n","      <td>[Hugo Touvron, Louis Martin, Kevin Stone, Pete...</td>\n","      <td>[cs.CL, cs.AI]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>cs.CL</td>\n","      <td>20230718</td>\n","      <td>20230719</td>\n","      <td>L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle : Open ...</td>\n","      <td>[{'id': '2302.13971'}, {'id': '1908.01091'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>421</th>\n","      <td>2211.05100</td>\n","      <td>BLOOM: A 176B-Parameter Open-Access Multilingu...</td>\n","      <td>Large language models (LLMs) have been shown t...</td>\n","      <td>http://arxiv.org/pdf/2211.05100</td>\n","      <td>[BigScience Workshop, :, Teven Le Scao, Angela...</td>\n","      <td>[cs.CL]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>cs.CL</td>\n","      <td>20221109</td>\n","      <td>20230627</td>\n","      <td>BLOOM: A 176B-Parameter Open-Access Multilingu...</td>\n","      <td>[{'id': '2208.07339'}, {'id': '2204.02311'}, {...</td>\n","    </tr>\n","    <tr>\n","      <th>422</th>\n","      <td>2009.10031</td>\n","      <td>Training Production Language Models without Me...</td>\n","      <td>This paper presents the first consumer-scale n...</td>\n","      <td>http://arxiv.org/pdf/2009.10031</td>\n","      <td>[Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews,...</td>\n","      <td>[cs.LG, cs.CR, stat.ML]</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>cs.LG</td>\n","      <td>20200921</td>\n","      <td>20200921</td>\n","      <td>1\\nTraining Production Language Models\\nwithou...</td>\n","      <td>[{'id': '2009.10031'}, {'id': '1610.05755'}, {...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>423 rows × 13 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1193d7d-d317-4ce9-8762-a594f3b3fe60')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e1193d7d-d317-4ce9-8762-a594f3b3fe60 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e1193d7d-d317-4ce9-8762-a594f3b3fe60');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-55839d62-9d8e-4ac5-8e32-540ca0bec5e9\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55839d62-9d8e-4ac5-8e32-540ca0bec5e9')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-55839d62-9d8e-4ac5-8e32-540ca0bec5e9 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_603979e1-40a2-45fc-b9fb-792bfd9861b2\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataset')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_603979e1-40a2-45fc-b9fb-792bfd9861b2 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('dataset');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"dataset","summary":"{\n  \"name\": \"dataset\",\n  \"rows\": 423,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 423,\n        \"samples\": [\n          \"1707.08819\",\n          \"2205.12255\",\n          \"1901.11117\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 423,\n        \"samples\": [\n          \"A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets\",\n          \"TALM: Tool Augmented Language Models\",\n          \"The Evolved Transformer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 423,\n        \"samples\": [\n          \"The original ImageNet dataset is a popular large-scale benchmark for training\\nDeep Neural Networks. Since the cost of performing experiments (e.g, algorithm\\ndesign, architecture search, and hyperparameter tuning) on the original dataset\\nmight be prohibitive, we propose to consider a downsampled version of ImageNet.\\nIn contrast to the CIFAR datasets and earlier downsampled versions of ImageNet,\\nour proposed ImageNet32$\\\\times$32 (and its variants ImageNet64$\\\\times$64 and\\nImageNet16$\\\\times$16) contains exactly the same number of classes and images as\\nImageNet, with the only difference that the images are downsampled to\\n32$\\\\times$32 pixels per image (64$\\\\times$64 and 16$\\\\times$16 pixels for the\\nvariants, respectively). Experiments on these downsampled variants are\\ndramatically faster than on the original ImageNet and the characteristics of\\nthe downsampled datasets with respect to optimal hyperparameters appear to\\nremain similar. The proposed datasets and scripts to reproduce our results are\\navailable at http://image-net.org/download-images and\\nhttps://github.com/PatrykChrabaszcz/Imagenet32_Scripts\",\n          \"Transformer based language models (LMs) demonstrate increasing performance\\nwith scale across a wide variety of tasks. Scale alone however cannot enable\\nmodels to solve tasks that require access to ephemeral, changing, or private\\ndata that was unavailable at training time. Many useful tasks may also benefit\\nfrom LMs being able to access APIs that read or modify state. In this work, we\\npresent Tool Augmented Language Models (TALM), combining a text-only approach\\nto augment language models with non-differentiable tools, and an iterative\\n\\\"self-play\\\" technique to bootstrap performance starting from few tool\\ndemonstrations. TALM exhibits strong performance on both a knowledge-heavy QA\\ntask and a reasoning oriented math task with simple tools. At a given model\\nscale, TALM significantly outperforms non-augmented LMs. We further demonstrate\\nthat TALM successfully performs out-of-distribution inferences on both QA and\\nmath tasks, where non-augmented LMs fail. Our results suggest that Tool\\nAugmented Language Models are a promising direction to enrich LMs'\\ncapabilities, with less dependence on scale.\",\n          \"Recent works have highlighted the strength of the Transformer architecture on\\nsequence tasks while, at the same time, neural architecture search (NAS) has\\nbegun to outperform human-designed models. Our goal is to apply NAS to search\\nfor a better alternative to the Transformer. We first construct a large search\\nspace inspired by the recent advances in feed-forward sequence models and then\\nrun evolutionary architecture search with warm starting by seeding our initial\\npopulation with the Transformer. To directly search on the computationally\\nexpensive WMT 2014 English-German translation task, we develop the Progressive\\nDynamic Hurdles method, which allows us to dynamically allocate more resources\\nto more promising candidate models. The architecture found in our experiments\\n-- the Evolved Transformer -- demonstrates consistent improvement over the\\nTransformer on four well-established language tasks: WMT 2014 English-German,\\nWMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size,\\nthe Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8\\non WMT'14 English-German; at smaller sizes, it achieves the same quality as the\\noriginal \\\"big\\\" Transformer with 37.6% less parameters and outperforms the\\nTransformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 423,\n        \"samples\": [\n          \"http://arxiv.org/pdf/1707.08819\",\n          \"http://arxiv.org/pdf/2205.12255\",\n          \"http://arxiv.org/pdf/1901.11117\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 231,\n        \"samples\": [\n          \"In the 57th Annual Meeting of the Association for Computational\\n  Linguistics (ACL). Florence, Italy. July 2019\",\n          \"Working in Process\",\n          \"8 pages, 6 figures. Accepted at the 2018 IEEE International\\n  Conference on Systems, Man, and Cybernetics (SMC2018)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"journal_ref\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"Transactions of the Association for Computational Linguistics\\n  (TACL), Vol 6 (2018), pages 287-302\",\n          \"ICML 2016\",\n          \"Published in Transactions on Machine Learning Research (TMLR),\\n  2023\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"primary_category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          \"cs.LG\",\n          \"cs.AI\",\n          \"stat.ML\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 365,\n        \"samples\": [\n          \"20200924\",\n          \"20220208\",\n          \"20220413\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"updated\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 360,\n        \"samples\": [\n          \"20210104\",\n          \"20200813\",\n          \"20201202\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 423,\n        \"samples\": [\n          \"ImageNet32x32, ImageNet16x16 and ImageNet64x64\\nA D OWNSAMPLED VARIANT OF IMAGE NET AS AN AL-\\nTERNATIVE TO THE CIFAR DATASETS\\nPatryk Chrabaszcz, Ilya Loshchilov & Frank Hutter\\nUniversity of Freiburg\\nFreiburg, Germany,\\nfchrabasp,ilya,fhg@cs.uni-freiburg.de\\nABSTRACT\\nThe original ImageNet dataset is a popular large-scale benchmark for training\\nDeep Neural Networks. Since the cost of performing experiments (e.g, algo-\\nrithm design, architecture search, and hyperparameter tuning) on the original\\ndataset might be prohibitive, we propose to consider a downsampled version of\\nImageNet. In contrast to the CIFAR datasets and earlier downsampled versions\\nof ImageNet, our proposed ImageNet32x32 (and its variants ImageNet64x64\\nand ImageNet16x16) contains exactly the same number of classes and images\\nas ImageNet, with the only difference that the images are downsampled to\\n32\\u000232 pixels per image (64 \\u000264 and 16\\u000216 pixels for the variants, respec-\\ntively). Experiments on these downsampled variants are dramatically faster\\nthan on the original ImageNet and the characteristics of the downsampled\\ndatasets with respect to optimal hyperparameters appear to remain similar.\\nThe proposed datasets and scripts to reproduce our results are available at\\nhttp://image-net.org/download-images and\\nhttps://github.com/PatrykChrabaszcz/Imagenet32_Scripts\\n1 I NTRODUCTION\\nDeep learning research has been substantially facilitated by the availability of realistic and acces-\\nsible benchmark datasets, such as CIFAR-10 and CIFAR-100 (Krizhevsky and Hinton, 2009) (and\\nMNIST (LeCun et al., 1998) in the 1990s). With the progress of machine learning, simple datasets\\nlose some of their relevance, and more complex datasets/tasks become more important. While good\\nresults can be achieved on more complex datasets, such as ImageNet (Krizhevsky et al., 2012; Rus-\\nsakovsky et al., 2015), this incurs a large computational burden, making it intractable to achieve\\nstate-of-the-art performance without massive compute resources (training a strong ImageNet model\\ntypically requires several GPU months).\\nDue to this computational expense of running experiments on the original ImageNet dataset we\\npropose to explore cheaper alternatives that preserve the dataset\\u2019s complexity. In order to check\\nthe scalability of new methods, neural architectures and hyperparameters associated with them, one\\nmight be interested in a downscaled version of ImageNet which allows for cheaper experimentation.\\nMoreover, a lower resolution of the images would make the classi\\ufb01cation task much more dif\\ufb01cult\\nand would thus postpone the saturation of benchmarking results currently observed on CIFAR-10,\\ne.g., 3% error obtained by Gastaldi (2017) compared to roughly 6% obtained by a trained human\\n(Karpathy, 2011).\\nTo address this issue, we provide downsampled variants of the original ImageNet dataset and analyze\\nresults on them w.r.t. different hyperparameter settings and network sizes. We obtain surprisingly\\nstrong classi\\ufb01cation results on our downsampled variants and \\ufb01nd qualitative results to be very\\nsimilar across downsampling sizes. This suggests that these downsampled datasets are useful for\\nfacilitating cheap experimentation.\\nThe basic contributions of this report are as follows:\\n1arXiv:1707.08819v3  [cs.CV]  23 Aug 2017\\nImageNet32x32, ImageNet16x16 and ImageNet64x64\\n\\u000fWe make available downsampled versions of ImageNet ( 64\\u000264,32\\u000232, and 16\\u000216pixels)\\nto facilitate fast experimentation with different network architectures, training algorithms,\\nand hyperparameters.\\n\\u000fWe show that different downsampling techniques yield similar results, except for a nearest\\nneighbor approach, which performed worse in all our experiments.\\n\\u000fUsing Wide ResNets (Zagoruyko and Komodakis, 2016), we obtain surprisingly good\\nperformance, matching the baseline by the pioneering AlexNet (Krizhevsky et al., 2012)\\n(18.2% top-5 error) while using ImageNet32x32 (whose images have roughly 50 \\u0002less\\npixels per image than the original ones).\\n\\u000fWe show that the range of optimal learning rates does not change much across Ima-\\ngeNet16x16, ImageNet32x32, and ImageNet64x64, as well as across different network\\nwidths. This could be exploited by multi-\\ufb01delity methods for architecture and hyperpa-\\nrameter search (Li et al., 2016; Klein et al., 2016).\\n2 D OWNSAMPLING IMAGE NET\\nThe original ImageNet dataset consists of images released as a part of the ILSVRC-2012 classi\\ufb01ca-\\ntion dataset (Krizhevsky et al., 2012; Russakovsky et al., 2015). Each image belongs to one of 1000\\nobject classes, with the number of training images per class varying from 732 to 1300; there are 50\\nvalidation images per class. The size of the original images varies; therefore, a preprocessing step is\\nusually applied to scale and crop images to the size of 224 \\u0002224 pixels.\\nWe are aware of two datasets that contain low resolution images derived from the ImageNet dataset:\\n\\u000fDownsampled ImageNet (Oord et al., 2016), like our datasets, contains all images in\\nImageNet, but since it was constructed for unsupervised learning, it does not provide the\\nactual image labels and can therefore not be used for supervised learning.\\n\\u000fTinyImageNet (available at https://tiny-imagenet.herokuapp.com/ ) con-\\ntains a subset of 200 classes with 500 images per class.\\nMishkin et al. (2016) suggested to use 128x128 pixels ImageNet images to evaluate various deep\\nlearning techniques, but their dataset is not available.\\nWe downsample / resize the original images to smaller images of 32x32 pixels to form Ima-\\ngeNet32x32, to images of 64x64 pixels to form ImageNet64x64 and to images of 16x16 pixels\\nto form ImageNet16x16. In contrast to TinyImageNet, we do not reduce the number of classes and\\nnumber of images. All images are shuf\\ufb02ed and then divided into 10 different \\ufb01les so that each \\ufb01le\\nis expected to have images from all classes. The validation data is stored in a separate \\ufb01le, both the\\ntraining and validation data points are labeled (e.g., indexed starting from 1) according to the map-\\nping \\ufb01le of the ImageNet devkit. Each \\ufb01le contains images, labels and the mean image computed\\nover the whole training set. We keep the same format of \\ufb01les as the one that is commonly used for\\nthe CIFAR datasets. ImageNet16x16, ImageNet32x32 and ImageNet64x64 take 1 GB, 4 GB and 16\\nGB of disk space, respectively.\\nWe consider 6 different downsampling techniques available in the Pillow library1: lanczos, nearest,\\nbilinear, bicubic, hamming, box (see Figure 1). In order to check the quality of the downsampled im-\\nages we use them to train Wide Residual Networks (WRNs) by Zagoruyko and Komodakis (2016),\\nexpecting that better validation errors will tend to be achieved with downsampling techniques that\\nlose less information.\\n3 E XPERIMENTAL SETUP\\nWe train Wide Residual Networks WRN-N-k by Zagoruyko and Komodakis (2016), where Nis the\\nnumber of layers and kis a multiplicative factor for the number of \\ufb01lters, with k= 1corresponding\\nto 16 \\ufb01lters in the \\ufb01rst residual block; increasing kmakes the network wider. We use Stochastic\\nGradient Descent with momentum factor 0.9, drop the learning rate by a factor of 5.0 every 10\\n1Pillow version 4.1 available at https://python-pillow.org\\n2\\nImageNet32x32, ImageNet16x16 and ImageNet64x64\\nepochs, and train up to a total budget of 40 epochs. Throughout, we show validation error rates\\nobtained after training for 31 epochs (right after the last drop of the learning rate).\\nOur experiments on ImageNet32x32 employ the original WRNs designed for the CIFAR datasets\\nwith 32\\u000232 pixels per image. To adapt WRNs for images with 64 \\u000264 pixels per image as used\\nin ImageNet64x64, we add an additional stack of residual blocks to reduce the spatial resolution of\\nthe last feature map from 16 \\u000216 to 8\\u00028 and thus double the number of features. Analogously, for\\nImageNet16x16, we remove the last stack of residual blocks. For data augmentation, we \\ufb02ip images\\nhorizontally and concatenate them to the original images, effectively doubling the number of images\\nper epoch. We also use random image shifts (up to 4 pixels horizontally and vertically).\\n4 R ESULTS\\nDoes the downsampling technique matter? We evaluated the six downsampling techniques de-\\nscribed in Section 2 using a small WRN-28-2 network and various initial learning rates LR2\\nf0:001;0:0025;0:005;0:01;0:025;0:05g. The results in Figure 2 show that all downsampling tech-\\nniques performed very similarly, except for the nearest neighbour technique which yielded the worst\\nresults for all learning rates. This observation is in line with Figure 1 and also holds for Ima-\\ngeNet16x16 and ImageNet64x64 (results not shown for brevity). For all remaining experiments in\\nthis paper, we used the box method.\\nDo conclusions drawn for cheap evaluations carry over to expensive ones? Next, we studied to\\nwhich extent conclusions drawn for small networks and downsampled images carry over to larger\\nnetworks and higher resolution images. This in turn determines the usefulness of these techniques\\nfor speeding up the experimental loop of architecture design and hyperparameter optimization. For\\nthis, we performed three experiments:\\n\\u000fWe studied how the results scale with the network size, more speci\\ufb01cally, network width,\\nde\\ufb01ned by k. Table 1 shows that larger kyielded better results independently of the\\ndownsampling size. Performance on our downsampled datasets was surprisingly strong;\\nfor example, on ImageNet32x32, using k= 10 achieved 40.96% Top-1 validation er-\\nror and 18.87% Top-5 validation error. Interestingly, this matches the original results by\\nAlexNets (Krizhevsky et al., 2012) (40.7% and 18.2%, respectively) on full-sized ImageNet\\n(which has roughly 50 times more pixels per image). Clearly, greater image resolution\\nyielded better results (e.g., 12.64% top-5 performance for ImageNet64x64).\\n\\u000fWe studied how optimal learning rates changed across different combinations of downsam-\\npling sizes and network widths. Figure 3 shows that the region of optimal learning rates\\nremained similar across all our experimental setups, including networks whose space and\\ntime complexity differed by up to a factor of 100. Additionally, Figure 4 compares perfor-\\nmance as a function of both learning rate and width multiplier kfor downsampling sizes\\nof 32x32 and 16x16, showing qualitatively very similar results in both cases, including the\\ninteraction effect that larger values of kfavor somewhat larger learning rates than smaller\\nFigure 1: The original images (\\ufb01rst column) and images obtained by 6 downsampling techniques\\n(left to right): bicubic, bilinear, box, hamming, lanczos, nearest. Our resizing procedure changes the\\naspect ratio of images.\\n3\\nImageNet32x32, ImageNet16x16 and ImageNet64x64\\n5 10 15 20 25 30 35 40\\nEpochs2030405060708090Top-5 Error (%)\\nLR=0.05, k=2\\nbicubic \\nbilinear \\nbox \\nhamming \\nlanczos \\nnearest \\n5 10 15 20 25 30 35 40\\nEpochs2030405060708090Top-5 Error (%)\\nLR=0.025, k=2\\nbicubic \\nbilinear \\nbox \\nhamming \\nlanczos \\nnearest \\n5 10 15 20 25 30 35 40\\nEpochs2030405060708090Top-5 Error (%)\\nLR=0.01, k=2\\nbicubic \\nbilinear \\nbox \\nhamming \\nlanczos \\nnearest \\n5 10 15 20 25 30 35 40\\nEpochs2030405060708090Top-5 Error (%)\\nLR=0.005, k=2\\nbicubic \\nbilinear \\nbox \\nhamming \\nlanczos \\nnearest \\n5 10 15 20 25 30 35 40\\nEpochs2030405060708090Top-5 Error (%)\\nLR=0.0025, k=2\\nbicubic \\nbilinear \\nbox \\nhamming \\nlanczos \\nnearest \\n5 10 15 20 25 30 35 40\\nEpochs2030405060708090Top-5 Error (%)\\nLR=0.001, k=2\\nbicubic \\nbilinear \\nbox \\nhamming \\nlanczos \\nnearest \\nFigure 2: The mean Top-5 errors obtained in 3 runs by WRN-28-2 on ImageNet32x32 for different\\nlearning rates (indicated at the top of each sub\\ufb01gure as LR) and downsampling algorithms.\\nk. This suggests that small networks and downsampled images may indeed facilitate faster\\nexperimentation.\\n\\u000fWe also investigated the tradeoffs of performance vs. training time resulting from dif-\\nferent downsampling and network sizes. Figure 5 and Table 1 show that both mecha-\\nnisms for reducing the computational cost should be considered simultaneously to achieve\\noptimal anytime performance. An additional mechanism could be to perform warm\\nrestarts (Loshchilov and Hutter, 2017), which was shown to substantially improve any-\\ntime performance over reductions of the learning rate at regular intervals. Since the relative\\nranking of learning rates was consistent across different downsampling and network sizes,\\nwe also envision that architecture and hyperparameter search methods could exploit cheap\\nproxies of computationally more expensive setups based on varying these degrees of free-\\ndom. Possible methods for exploiting these include Li et al. (2016); Klein et al. (2016).\\n4\\nImageNet32x32, ImageNet16x16 and ImageNet64x64\\nwidth k# params Top-1 error Top-5 error Time [days]\\nWRN-20-k on ImageNet16x16 1 0.12M 85,18% 66,12% 0.2\\nWRN-20-k on ImageNet16x16 2 0.42M 77,00% 54,22% 0.4\\nWRN-20-k on ImageNet16x16 5 2.3M 66,60% 41,59% 1.0\\nWRN-20-k on ImageNet16x16 10 8.9M 59.94% 35.10% 2.7\\nWRN-28-k on ImageNet32x32 0.5 0.13M 79,83% 57,64% 0.5\\nWRN-28-k on ImageNet32x32 1 0.44M 67,97% 42,49% 0.8\\nWRN-28-k on ImageNet32x32 2 1.6M 56,92% 30,92% 1.5\\nWRN-28-k on ImageNet32x32 5 9.5M 45,36% 21,36% 4.9\\nWRN-28-k on ImageNet32x32 10 37.1M 40,96% 18,87% 13.8\\nWRN-36-k on ImageNet64x64 0.5 0.44M 62,35% 36,06% 2.1\\nWRN-36-k on ImageNet64x64 1 1.6M 49,79% 24,17% 3.4\\nWRN-36-k on ImageNet64x64 2 6.2M 39,55% 16,57% 6.4\\nWRN-36-k on ImageNet64x64 5 37.6M 32,34% 12,64% 22\\nTable 1: The mean Top-1 and Top-5 test error rates obtained in 3 runs by WRNs measured right\\nafter the last drop of the learning rate, i.e., after epoch 31 (for bigger models training for more than\\none epoch after the last drop can lead to over\\ufb01tting). All results are based on a learning rate of 0.01.\\nThe timing results are reported for training on a single Titan X GPU.\\n0.05 0.025 0.01 0.005 0.0025 0.001\\nLearning Rate102030405060708090Top-1 Error (%)\\nk=0.5\\nk=1k=2\\nk=5k=10\\n16x1632x32\\n64x64\\n0.05 0.025 0.01 0.005 0.0025 0.001\\nLearning Rate102030405060708090Top-5 Error (%)\\nk=0.5\\nk=1k=2\\nk=5k=10\\n16x1632x32\\n64x64\\nFigure 3: The mean Top-1 (Left) and Top-5 (Right) errors obtained in 3 runs by WRN-N- kafter\\n31 epochs with different settings of the initial learning rates and different sizes of the downsampled\\nimages (ImageNet16x16, ImageNet32x32 and ImageNet64x64). The results for ImageNet64x64 are\\nshown for different kbut a single value of the initial learning rate LR=0.01 which seems reasonably\\ngood across different settings.\\nFigure 4: The mean Top-5 errors for 32x32 images (left) and 16x16 images (right), as a function of\\nnetwork width kand learning rate.\\n5\\nImageNet32x32, ImageNet16x16 and ImageNet64x64\\n101\\n102\\n103\\nTraining time (hours)01020304050607080Top 5 Error (%)126k\\n435k\\n1.6M\\n9.5M37.1M438k\\n1.6M\\n6.2M\\n37.6M123k\\n416k2.3M\\n8.9M16x16 Images\\n32x32 Images\\n64x64 Images\\n101\\n102\\n103\\nTraining time (hours)01020304050607080Top 5 Error (%)126k\\n435k\\n1.6M\\n9.5M37.1M438k\\n1.6M\\n6.2M\\n37.6M123k\\n416k2.3M\\n8.9M16x16 Images\\n32x32 Images\\n64x64 Images\\nFigure 5: The mean Top-5 test error rates according to Table 1 for different models (with different\\nnumber of parameters) vs. training time on a single Titan X GPU. The bottom \\ufb01gure replicates the\\ntop one, but also shows semi-transparent curves in the background to represent convergence curves.\\n6\\nImageNet32x32, ImageNet16x16 and ImageNet64x64\\n0 200 400 600 800 1000\\nClass020406080100Top-1 Validation accuracy (%)\\n0 200 400 600 800 1000\\nClass020406080100Top-5 Validation accuracy (%)\\nFigure 6: Percentage of correct Top-1 (Left) and Top-5 (Right) predictions for different classes ob-\\ntained by WRN-28-5 on ImageNet32x32. Classes are ordered by this value for better visualization.\\n5 D ISCUSSION AND CONCLUSION\\nOur proposed downsampled versions of the original ImageNet dataset might represent a viable alter-\\nnative to the CIFAR datasets while dealing with more complex data and classes. Quite surprisingly,\\neven by greatly reducing the resolution of images to 32 \\u000232 pixels, one can predict image labels\\nquite well (see also Figure 6 and Figure 7).\\nClassi\\ufb01cation of low resolution images might also be of interest when (i) data storage is important\\n(the original ImageNet dataset is 145GB), (ii) the input images are corrupted by noise, or (iii) a\\nsmall subpart of a high resolution image must be classi\\ufb01ed.\\nWe hope that the provided datasets will \\ufb01ll the gap between the CIFAR datasets and the full Im-\\nageNet dataset, representing a good benchmark for experimental studies, such as algorithm de-\\nsign, neural network architecture search and hyperparameter optimization. Our preliminary experi-\\nments support the hypothesis that \\ufb01ndings obtained on smaller networks for lower resolution images\\nmay transfer to larger networks for higher resolution images, while being up to 100 times cheaper\\nto obtain. This could be exploited by multi-\\ufb01delity methods for architecture and hyperparameter\\nsearch (Li et al., 2016; Klein et al., 2016).\\n6 A CKNOWLEDGEMENT\\nThis work has partly been supported by the European Research Council (ERC) under the Euro-\\npean Unions Horizon 2020 research and innovation programme under grant no. 716721 and by\\nthe German Research Foundation (DFG), under the BrainLinksBrainTools Cluster of Excellence\\n(grant number EXC 1086). The authors acknowledge support by the High Performance and Cloud\\nComputing Group at the Zentrum f \\u00a8ur Datenverarbeitung of the University of T \\u00a8ubingen, the state of\\nBaden-W \\u00a8urttemberg through bwHPC and the German Research Foundation (DFG) through grant\\nno INST 37/935-1 FUGG.\\nBIBLIOGRAPHY\\nXavier Gastaldi. Shake-shake regularization of 3-branch residual networks. In 5th International\\nConference on Learning Representations (ICLR 2017) , 2017.\\nAndrej Karpathy. Lessons learned from manually classifying CIFAR-10, 2011. URL http:\\n//karpathy.github.io/2011/04/27/manually-classifying-cifar10/ . Ac-\\ncessed: 2017-05-19.\\nAaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast\\nbayesian optimization of machine learning hyperparameters on large datasets. arXiv preprint\\narXiv:1605.07079 , 2016.\\n7\\nImageNet32x32, ImageNet16x16 and ImageNet64x64\\nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi\\ufb01cation with deep convolu-\\ntional neural networks. In Advances in neural information processing systems , pages 1097\\u20131105,\\n2012.\\nYann LeCun, L \\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\\ndocument recognition. Proceedings of the IEEE , 86(11):2278\\u20132324, 1998.\\nLisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hy-\\nperband: A novel bandit-based approach to hyperparameter optimization. arXiv preprint\\narXiv:1603.06560 , 2016.\\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic Gradient Descent with Restarts. In Interna-\\ntional Conference on Learning Representations (ICLR 2017) , 2017.\\nDmytro Mishkin, Nikolay Sergievskiy, and Jiri Matas. Systematic evaluation of CNN advances on\\nthe ImageNet. arXiv preprint arXiv:1606.02228 , 2016.\\nAaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\\narXiv preprint arXiv:1601.06759 , 2016.\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.\\nImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision\\n(IJCV) , 115(3):211\\u2013252, 2015. doi: 10.1007/s11263-015-0816-y.\\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint\\narXiv:1605.07146 , 2016.\\n8\\nImageNet32x32, ImageNet16x16 and ImageNet64x64\\nFigure 7: Subset of Imagenet32x32 validation images from classes with 1th (trolleybus, 100% ac-\\ncuracy), 250th (sloth bear, 88% accuracy), 500th (bulbul, 80% accuracy), 750th (projectile, 70%\\naccuracy) and 1000th (plastic bag, 26% accuracy) best Top-5 accuracy as given in Figure 6. Green\\nimage borders indicate correct Top-5 predictions. The results were obtained by WRN-28-5 on Ima-\\ngeNet32x32.\\n9\",\n          \"TALM: Tool Augmented Language Models\\nAaron Parisi Yao Zhao Noah Fiedel\\nfaarontp ;yaozhaoyz ;nfiedelg\\n@google.com\\nAbstract\\nTransformer based language models (LMs)\\ndemonstrate increasing performance with\\nscale across a wide variety of tasks. Scale\\nalone however cannot enable models to solve\\ntasks that require access to ephemeral, chang-\\ning, or private data that was unavailable at\\ntraining time. Many useful tasks may also ben-\\ne\\ufb01t from LMs being able to access APIs that\\nread or modify state. In this work, we present\\nTool Augmented Language Models (TALM),\\ncombining a text-only approach to augment\\nlanguage models with non-differentiable tools,\\nand an iterative \\u201cself-play\\u201d technique to boot-\\nstrap performance starting from few tool\\ndemonstrations. TALM exhibits strong per-\\nformance on both a knowledge-heavy QA task\\nand a reasoning oriented math task with sim-\\nple tools. At a given model scale, TALM sig-\\nni\\ufb01cantly outperforms non-augmented LMs.\\nWe further demonstrate that TALM success-\\nfully performs out-of-distribution inferences\\non both QA and math tasks, where non-\\naugmented LMs fail. Our results suggest\\nthat Tool Augmented Language Models are a\\npromising direction to enrich LMs\\u2019 capabili-\\nties, with less dependence on scale.\\n1 Introduction\\nLanguage models using the Transformer architec-\\nture [Vaswani et al., 2017] demonstrate increas-\\ning performance at larger scales, e.g. T5 [Raf-\\nfel et al., 2019], GPT-3 [Brown et al., 2020], and\\nPaLM [Chowdhery et al., 2022]. Scale related per-\\nformance gains are observed on a variety of bench-\\nmarks, e.g. SuperGLUE [Wang et al., 2019] and\\nBIG-bench [BIG-bench collaboration, 2021].\\nScaling up has practical downsides. Large scale\\nmodels are unwieldy to store, transfer, and deploy.\\nTheir costs to train or perform inference can be\\nprohibitively high for many researchers and orga-\\nnizations.\\n0 1 2 3\\nSelf-play Round152025303540Natural Questions (F1)\\nKnowledge\\nTALM (xl)\\nTALM (large)\\nTALM (base)\\nLM (xl)\\nLM (large)\\nLM (base)\\n0 1 2 3\\nSelf-play Round2025303540455055MathQA (solver-match)\\nReasoning\\nTALM (xl)\\nTALM (large)\\nTALM (base)\\nLM (xl)\\nLM (large)\\nLM (base)Figure 1: Baseline LM and TALM performance on two\\ntasks, with increasing rounds of self-play.\\nLarger models memorize more world knowl-\\nedge [Roberts et al., 2020]. While good for many\\nbenchmark tasks, relying on memorization alone\\nposes several problems. First, models sometimes\\ngenerate incorrect outputs that are problematic for\\nsome applications. Second, world knowledge is\\nconstantly changing. The knowledge from yester-\\nday\\u2019s training data might be invalid today. Third,\\nlarge models can memorize parts of their training\\ndata with undesirable consequences [Carlini et al.,\\n2022].\\nRetrieval based approaches to enhancing LMs\\ncan lower the dependence on scale. REALM [Guu\\net al., 2020] learns retrieval via backpropagation\\nfrom a \\ufb01xed corpus. RETRO [Borgeaud et al.,\\n2021] adds an \\u201dinternet scale\\u201d retrieval mecha-\\nnism. RAG [Lewis et al., 2020] uses a dense vec-\\ntor index of Wikipedia, and retrieves either once\\nper token or once per query. Other works demon-\\nstrated that LMs can be enhanced on math reason-\\ning with access to a calculator [Andor et al., 2019].\\nLooking towards the future utility of language\\nmodels, it is clear that scale and retrieval cannot\\nsolve all useful problems. Many knowledge tasks\\nand desirable applications require access to read\\nlive or private data (e.g. weather or a person\\u2019s cal-arXiv:2205.12255v1  [cs.CL]  24 May 2022\\nendar), or to invoke APIs that modify state. Recent\\nworks such as Say Can [Ahn et al., 2022] connect\\nlanguages models to an environment, though with\\nthe model as a recipient of queries. In contrast,\\nTALM\\u2019s approach enables models to invoke arbi-\\ntrary tools with model-generated output, and to at-\\ntend to tool output to generate task outputs.\\nIn summary, our contributions are:\\n\\u2022 Demonstrating that language models can be\\naugmented with tools via a text-to-text API.\\n\\u2022 Demonstrating an iterative self-play tech-\\nnique to bootstrap tool-augmented datasets\\nand subsequent tool-augmented model per-\\nformance, from few labeled examples.\\n2 Methods\\nWe use pretrained T5 models [Raffel et al., 2019,\\nRoberts et al., 2022] for \\ufb01netuning, inference and\\nevaluation. To measure the effects of model scal-\\ning, we use the base, large, and XL sizes.\\n2.1 Tool Augmented Language Models\\ninput output tool input tool resultLanguage Model\\nTool Augmented Language Modelinput output\\ncall\\nexternal\\ntoolappend\\ntool\\nresult\\nFigure 2: LM and Tool Augmented LMs.\\nWe use a Text-to-Text tool interface given its\\nbroad applicability and simplicity, as shown in\\nFig. 3. TALM \\ufb01rst generates a tool input condi-\\ntioned on the task input text and invokes a tool\\u2019s\\nAPI by generating a delimiter, such as \\u201d jresult \\u201d.\\nWhenever this delimiter is detected, the tool API is\\ncalled and its result appended to the text sequence.\\nTALM then continues to generate the \\ufb01nal task\\noutput.\\nAn abstract task:\\ntask input text jtool-call tool input text jresult tool out-\\nput text joutput task output text\\nA weather task:\\nhow hot will it get in NYC today? jweather lookup re-\\ngion=NYC jresult precipitation chance: 10, high temp:\\n20c, low-temp: 12c joutput today\\u2019s high will be 20C\\nFigure 3: TALM text-to-text interface example.TALM learns two subtasks at the same time:\\ncalling a tool and generating an answer based on\\ntool results. TALM is architecturally agnostic and\\ncan be implemented as Seq2Seq, left-to-right LM\\nor pre\\ufb01x LM. We chose the Seq2Seq family for its\\nhigh \\ufb01netuning performance at modest scale [Raf-\\nfel et al., 2019].\\n2.2 Iterative self-play\\nWhen introducing new tools to solve existing\\ntasks, there are often a limited number of demon-\\nstrations of tool interactions. However, there is\\ntypically plenty of supervised task data consist-\\ning of input and target pairs, and automated met-\\nrics for evaluating the correctness of a generated\\noutput. Inspired by Decision Transformer [Chen\\net al., 2021], we use a self-play approach to iter-\\natively bootstrap examples of tool-use with pro-\\ngressively higher quality. In this work, we refer\\nto a model interacting with a tool API as self-play\\nrather than adversarial play among models.\\nAlgorithm 1 Iterative Self-Play Algorithm.\\nx: task input,y: task output, t: tool input,r: tool output\\n1:T=fxi;yigT # task set\\n2:D=fxj;tj;rj;yjgD # tool-use set\\n3:P\\u0012 pretrainedLM\\n4:fort2[0;1;:::;R ]do # self-play rounds\\n5: # \\ufb01netune LM\\n6:\\u0012 argmax\\n\\u0012Q\\nDP\\u0012(yjjxj;tj;rj)P\\u0012(tjjxj)\\n7: forxi;yi2Tdo # iterate task set\\n8: forn2[0;1;:::;N ]do\\n9:tn P\\u0012(tjxi) # sample tool query\\n10:rn Tool(tn) # call tool API\\n11:yn P\\u0012(yjxi;tn;rn) # get task output\\n12: ifjyn\\u0000yij<th then # \\ufb01lter wrong output\\n13: D D[fxi;tn;rn;yng1\\n14: # update tool-use set\\nThe iterative self-play pipeline starts with a\\nsmall tool-use bootstrapping set fxj; tj; rj; yjgD.\\nIn each round of self-play, the TALM is \\ufb01ne-tuned\\non the tool-use set D. Next, for every example in\\nthe task set T, the TALM samples tool inputs, calls\\na tool API, and samples task outputs based on the\\ntool results. If the TALM generated task output\\nmatches the target within some threshold th, the\\ntool-use sequence led to the result is added to the\\ntool-use set Dfor the next round of self-play.\\nTo explore diverse tool API invocations and an-\\nswers during self-play, the TALM decodes using\\nrandom sampling with temperature=1.0, and top-\\nk=40. To grow the dataset during self-play, the\\nTALM generates up to N=600 tool-use sequences\\nper example. At evaluation time, the model uses\\nbeam decoding with 4 beams to generate a single\\noutput.\\nWe note that this iterative self-play pipeline rep-\\nresents a special case of a policy-gradient RL al-\\ngorithm, where the LM is the policy network and\\nis trained by policy gradient with a binary reward\\nsignal. Iterative self-play is related to expert it-\\neration [Anthony et al., 2017], which has been\\ndemonstrated to work well in tasks with extremely\\nweak supervision [Christiano et al., 2018]. While\\nour tasks are currently single-hop, this formulation\\ncan be extended further into RL: modelling multi-\\nhop tool-use tasks as markov decision processes\\n(MDPs), or integrating algorithms like Decision\\nTransformer [Chen et al., 2021].\\n3 Results\\nWe evaluate TALM on two domains. The \\ufb01rst is\\nthe knowledge-oriented Natural Questions (NQ)\\n[Kwiatkowski et al., 2019], a diverse QA task. The\\nsecond is MathQA [Amini et al., 2019], selected to\\nmeasure general reasoning capability rather than\\nknowledge.\\n3.1 Natural Questions\\nNatural Questions (NQ) is a large ( \\u0019300k train-\\ning examples) QA dataset collected from real user\\nqueries. NQ contains both long and short an-\\nswer tasks. We selected the short answer task\\nas it is both more challenging as measured with\\nlower baseline performance, and closer to practi-\\ncal use-cases such as assistants. In addition to a\\nquestion and short-answer pair, examples in the\\nNQ dataset include an \\u201doracle\\u201d context (span) of\\na Wikipedia document containing the answer. We\\nremove boolean questions to avoid in\\ufb02ated perfor-\\nmance due to random-chance guesses. We com-\\npare TALM against closed-book LM benchmarks.\\nFor TALM experiments, we do not feed the or-\\nacle contexts directly to the model, instead using\\nthem to populate an index that TALM can access\\nas a retrieval tool. The retrieval system is imple-\\nmented using a BM25-based index over the union\\nof all NQ oracle contexts.\\nIn Fig. 5, even the 220Mbase TALM outper-\\nforms 3BXL LM. There is also a smaller perfor-\\nmance gap between base and XL sized TALMs\\nthan between TALM and LM, suggesting thatQuestion: when are hops added in brewing process?\\nShort Answer: The boiling process.\\njquestion when are hops added in brewing process?\\njsearch brewing process jresult The boiling process is\\nwhere chemical reactions take place...including joutput\\nThe boiling process.\\nFigure 4: Example from Natural Questions, as a stan-\\ndard NQ task and the corresponding tool-augmented\\nsequence.\\n220M 770M 3000M\\nParameters323436384042Natural Questions (F1)\\nKnowledge\\nTALM\\nLM\\nFigure 5: Performance of TALM compared with\\nTALM of different model sizes on Natural Questions.\\nThe TALM is bootstrapped from 150 tool demonstra-\\ntions and undergoes two rounds of self-plays. We\\nhypothesize that the noise in the performance-scale\\ncurves is due to \\ufb01netuning in a low-data regime.\\nsmaller models bene\\ufb01t more from retrieval tools\\nfor knowledge intensive tasks.\\n3.2 MathQA\\nMathQA [Amini et al., 2019] is a large scale\\ndataset of math word problems ( \\u001930ktraining\\nexamples). Each example includes the word prob-\\nlem, a formula generated by crowd-source work-\\ners to calculate the answer, and the correct text-\\nform answer among multiple choices.\\nQuestion: If Lily\\u2019s test scores are 85 , 88 and 95 out\\nof 100 in 3 different subjects , what will be her average\\nscore?\\nFormula: Divide(Add(85, Add(88, 95)), 3)\\nAnswer: 89.33\\njquestion If Lily\\u2019s test scores are 85 , 88 and 95 out\\nof 100 in 3 different subjects , what will be her aver-\\nage score?jformula Divide(Add(85, Add(88, 95)), 3)\\njresult 89.3333333333 joutput 89.33\\nFigure 6: Example from MathQA, as a standard\\nMathQA task and the corresponding tool-augmented\\nsequence..\\nWe implemented a simple solver tool to exe-\\ncute formulas and check their results\\u2019 correctness\\nagainst their associated text-form answers. Ac-\\ncording to our solver tool, approximately 70%\\nof the formulas in MathQA produce results that\\nmatch their corresponding answers, similar to the\\n\\ufb01ndings in [Hendrycks et al., 2021]. Our man-\\nual inspections show that mismatched results are\\ndue to either wrong formulas or invalid answers.\\nThe bootstrap tool-use dataset consists of a ran-\\ndom sample of 10% of the training corpus where\\nthe formula is valid ( \\u00192kexamples). The TALM\\nsigni\\ufb01cantly outperforms a non-augmented LM as\\nshown in Fig. 7.\\n220M 770M 3000M\\nParameters2025303540455055MathQA (solver-match)\\nReasoning\\nTALM\\nLM\\nFigure 7: Performance of TALM compared with LM of\\ndifferent model sizes on MathQA.\\n3.3 Self-Play Ablations\\nWe \\ufb01nd that TALMs perform signi\\ufb01cantly better\\nafter a single round of self-play than after training\\nonly on the limited bootstrap tool-use training ex-\\namples, as shown in Fig. 1. Their performance\\ncontinues to increase over three rounds of self-\\nplay. This trend holds across model sizes ranging\\nfrom 220Mto3B.\\n3.4 Out-of-distribution Examples\\nOne bene\\ufb01t of TALM is its capability to general-\\nize to input text that is out-of-distribution to the\\nmodel\\u2019s training data, yet solvable with access to\\ntools.\\nOn the knowledge-heavy QA task, we replace\\nthe BM-25 Wiki retriever with a public search\\nengine, and show that TALM handles changing\\nworld knowledge well (see Fig. 8).\\nQuestion: What is wordle?\\nLM: a word generator\\nTALM: a simple online word game that challenges\\npeople to \\ufb01nd a \\ufb01ve-letter word in six guesses\\nFigure 8: LM vs TALM on changing knowledge.On the math task, we test large number han-\\ndling, an area where training data is lacking and\\nnon-augmented LMs are known to perform poorly\\n[Brown et al., 2020]. See Fig. 9 demonstrating\\nthat TALM can handle large numbers, where a LM\\ndoes not.\\nQuestion: A car is driving 535 miles per hour, how\\nmany hours does it take to travel 2450 miles?\\nLM: 8.5\\nTALM: 4.58\\nFigure 9: LM vs TALM on a large number operation.\\n4 Conclusion\\nIn this paper we present TALM, a framework for\\naugmenting language models with arbitrary tools.\\nTALM has two key ideas. First, we model tool-\\nuse via a text-to-text interface. Second, we apply\\nan iterative self-play technique to bootstrap high\\nperformance on tasks with few tool-use labelled\\nexamples. Taken together, this interface and tech-\\nnique make exploring additional tools and tasks\\npossible, without requiring expensive data label-\\ning efforts.\\nTALM consistently outperforms a non-\\naugmented LM on both a knowledge task (NQ)\\nand reasoning task (MathQA). Ablations show\\nthat self-play is key to good performance, and that\\niterative self-play yields further gains. We con-\\nclude that the combination of tool augmentation\\nand iterative self-play enables smaller models to\\noutperform larger non-augmented LMs.\\nWe hope that this work enables further research\\ninto tool augmented language models, a promising\\ndirection to enhance model capabilities with less\\ndependency on scale than many contemporary ap-\\nproaches.\\nReferences\\nMichael Ahn, Anthony Brohan, Noah Brown, Yev-\\ngen Chebotar, Omar Cortes, Byron David, Chelsea\\nFinn, Keerthana Gopalakrishnan, Karol Hausman,\\nAlex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz,\\nBrian Ichter, Alex Irpan, Eric Jang, Rosario Jau-\\nregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J\\nJoshi, Ryan Julian, Dmitry Kalashnikov, Yuheng\\nKuang, Kuang-Huei Lee, Sergey Levine, Yao Lu,\\nLinda Luu, Carolina Parada, Peter Pastor, Jor-\\nnell Quiambao, Kanishka Rao, Jarek Rettinghouse,\\nDiego Reyes, Pierre Sermanet, Nicolas Sievers,\\nClayton Tan, Alexander Toshev, Vincent Van-\\nhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and\\nMengyuan Yan. Do as i can, not as i say: Ground-\\ning language in robotic affordances, 2022. URL\\nhttps://arxiv.org/abs/2204.01691 .\\nAida Amini, Saadia Gabriel, Peter Lin, Rik\\nKoncel-Kedziorski, Yejin Choi, and Han-\\nnaneh Hajishirzi. Mathqa: Towards inter-\\npretable math word problem solving with\\noperation-based formalisms, 2019. URL\\nhttps://arxiv.org/abs/1905.13319 .\\nDaniel Andor, Luheng He, Kenton Lee, and Emily\\nPitler. Giving bert a calculator: Finding operations\\nand arguments with reading comprehension, 2019.\\nThomas Anthony, Zheng Tian, and David Barber.\\nThinking fast and slow with deep learning and tree\\nsearch. CoRR , abs/1705.08439, 2017. URL http:\\n//arxiv.org/abs/1705.08439 .\\nBIG-bench collaboration. Beyond the imitation game:\\nMeasuring and extrapolating the capabilities of lan-\\nguage models. In preparation , 2021. URL https:\\n//github.com/google/BIG-bench/ .\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\\nGeorge van den Driessche, Jean-Baptiste Lespiau,\\nBogdan Damoc, Aidan Clark, Diego de Las Casas,\\nAurelia Guy, Jacob Menick, Roman Ring, Tom Hen-\\nnigan, Saffron Huang, Loren Maggiore, Chris Jones,\\nAlbin Cassirer, Andy Brock, Michela Paganini, Ge-\\noffrey Irving, Oriol Vinyals, Simon Osindero, Karen\\nSimonyan, Jack W. Rae, Erich Elsen, and Laurent\\nSifre. Improving language models by retrieving\\nfrom trillions of tokens, 2021.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\\nGretchen Krueger, Tom Henighan, Rewon Child,\\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\\nClemens Winter, Christopher Hesse, Mark Chen,\\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\\nChess, Jack Clark, Christopher Berner, Sam Mc-\\nCandlish, Alec Radford, Ilya Sutskever, and Dario\\nAmodei. Language models are few-shot learn-\\ners, 2020. URL https://arxiv.org/abs/\\n2005.14165 .\\nNicholas Carlini, Daphne Ippolito, Matthew Jagiel-\\nski, Katherine Lee, Florian Tramer, and Chiyuan\\nZhang. Quantifying memorization across neural\\nlanguage models, 2022. URL https://arxiv.\\norg/abs/2202.07646 .\\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,\\nAditya Grover, Michael Laskin, Pieter Abbeel, Ar-\\navind Srinivas, and Igor Mordatch. Decision trans-\\nformer: Reinforcement learning via sequence mod-\\neling, 2021. URL https://arxiv.org/abs/\\n2106.01345 .Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton,\\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\\nHutchinson, Reiner Pope, James Bradbury, Jacob\\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\\nGarcia, Vedant Misra, Kevin Robinson, Liam Fedus,\\nDenny Zhou, Daphne Ippolito, David Luan, Hyeon-\\ntaek Lim, Barret Zoph, Alexander Spiridonov, Ryan\\nSepassi, David Dohan, Shivani Agrawal, Mark\\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\\nErica Moreira, Rewon Child, Oleksandr Polo-\\nzov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele\\nCatasta, Jason Wei, Kathy Meier-Hellstern, Dou-\\nglas Eck, Jeff Dean, Slav Petrov, and Noah\\nFiedel. Palm: Scaling language modeling with path-\\nways, 2022. URL https://arxiv.org/abs/\\n2204.02311 .\\nPaul F. Christiano, Buck Shlegeris, and Dario Amodei.\\nSupervising strong learners by amplifying weak ex-\\nperts. CoRR , abs/1810.08575, 2018. URL http:\\n//arxiv.org/abs/1810.08575 .\\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\\nsupat, and Ming-Wei Chang. Realm: Retrieval-\\naugmented language model pre-training, 2020.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\\nArora, Steven Basart, Eric Tang, Dawn Song, and\\nJacob Steinhardt. Measuring mathematical prob-\\nlem solving with the MATH dataset. CoRR ,\\nabs/2103.03874, 2021. URL https://arxiv.\\norg/abs/2103.03874 .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\n\\ufb01eld, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. Natural ques-\\ntions: a benchmark for question answering research.\\nTransactions of the Association of Computational\\nLinguistics , 2019.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\\nrich K \\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim\\nRockt \\u00a8aschel, Sebastian Riedel, and Douwe Kiela.\\nRetrieval-augmented generation for knowledge-\\nintensive nlp tasks, 2020. URL https://\\narxiv.org/abs/2005.11401 .\\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\\nine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J. Liu. Exploring the lim-\\nits of transfer learning with a uni\\ufb01ed text-to-text\\ntransformer, 2019. URL https://arxiv.org/\\nabs/1910.10683 .\\nAdam Roberts, Colin Raffel, and Noam Shazeer. How\\nmuch knowledge can you pack into the parame-\\nters of a language model?, 2020. URL https:\\n//arxiv.org/abs/2002.08910 .\\nAdam Roberts, Hyung Won Chung, Anselm Levskaya,\\nGaurav Mishra, James Bradbury, Daniel Andor,\\nSharan Narang, Brian Lester, Colin Gaffney, Afroz\\nMohiuddin, et al. Scaling up models and data with\\nt5x and seqio. arXiv preprint arXiv:2203.17189 ,\\n2022.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, \\u0141ukasz\\nKaiser, and Illia Polosukhin. Attention is all you\\nneed, 2017.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel R. Bowman. Superglue: A stick-\\nier benchmark for general-purpose language under-\\nstanding systems, 2019. URL https://arxiv.\\norg/abs/1905.00537 .\\n5 Appendix\\n5.1 Acknowledgements\\nThe authors would like to thank Noam Shazeer\\nfor early brainstorming on the path towards this\\nwork. We also thank Igor Mordatch for discus-\\nsions and feedback. Finally we thank Mohammad\\nSaleh for his helpful review and feedback improv-\\ning this manuscript.\\n5.2 Author Contributions\\nThis section lists the author contributions of each\\nauthor.\\n\\u2022 Aaron Parisi designed and implemented tool-\\naugmentation and self-play pipelines. Aaron\\nran the vast majority of experiments, and par-\\nticipated in brainstorming and paper writing.\\n\\u2022 Yao Zhao participated in brainstorming, ex-\\nperimental setup discussion and paper writ-\\ning. Yao implemented NQ/mathQA baselines\\nand mathQA solvers.\\n\\u2022 Noah Fiedel conceived of the project, par-\\nticipated in brainstorming, led the research\\ngroup and writing the paper.\",\n          \"The Evolved Transformer\\nDavid R. So1Chen Liang1Quoc V . Le1\\nAbstract\\nRecent works have highlighted the strength of\\ntheTransformer architecture on sequence tasks\\nwhile, at the same time, neural architecture search\\n(NAS) has begun to outperform human-designed\\nmodels. Our goal is to apply NAS to search for\\na better alternative to the Transformer. We \\ufb01rst\\nconstruct a large search space inspired by the re-\\ncent advances in feed-forward sequence models\\nand then run evolutionary architecture search with\\nwarm starting by seeding our initial population\\nwith the Transformer. To directly search on the\\ncomputationally expensive WMT 2014 English-\\nGerman translation task, we develop the Progres-\\nsive Dynamic Hurdles method, which allows us\\nto dynamically allocate more resources to more\\npromising candidate models. The architecture\\nfound in our experiments \\u2013 the Evolved Trans-\\nformer \\u2013 demonstrates consistent improvement\\nover the Transformer on four well-established\\nlanguage tasks: WMT 2014 English-German,\\nWMT 2014 English-French, WMT 2014 English-\\nCzech and LM1B. At a big model size, the\\nEvolved Transformer establishes a new state-of-\\nthe-art BLEU score of 29.8 on WMT\\u201914 English-\\nGerman; at smaller sizes, it achieves the same\\nquality as the original \\u201dbig\\u201d Transformer with\\n37.6% less parameters and outperforms the Trans-\\nformer by 0.7 BLEU at a mobile-friendly model\\nsize of\\u00187M parameters.\\n1. Introduction\\nOver the past few years, impressive advances have been\\nmade in the \\ufb01eld of neural architecture search. Reinforce-\\nment learning and evolution have both proven their capacity\\nto produce models that exceed the performance of those\\ndesigned by humans (Real et al., 2019; Zoph et al., 2018).\\nThese advances have mostly focused on improving vision\\n1Google Research, Brain Team, Mountain View, California,\\nUSA. Correspondence to: David R. So <davidso@google.com >.\\nProceedings of the 36thInternational Conference on Machine\\nLearning , Long Beach, California, PMLR 97, 2019. Copyright\\n2019 by the author(s).models, although some effort has also been invested in\\nsearching for sequence models (Zoph & Le, 2017; Pham\\net al., 2018). In these cases, it has always been to \\ufb01nd\\nimproved recurrent neural networks (RNNs), which were\\nlong established as the de facto neural model for sequence\\nproblems (Sutskever et al., 2014; Bahdanau et al., 2015).\\nHowever, recent works have shown that there are better al-\\nternatives to RNNs for solving sequence problems. Due to\\nthe success of convolution-based networks, such as Con-\\nvolution Seq2Seq (Gehring et al., 2017), and full attention\\nnetworks, such as the Transformer (Vaswani et al., 2017),\\nfeed-forward networks are now a viable option for solving\\nsequence-to-sequence (seq2seq) tasks. The main strength\\nof feed-forward networks is that they are faster, and easier\\nto train than RNNs.\\nThe goal of this work is to examine the use of neural ar-\\nchitecture search methods to design better feed-forward\\narchitectures for seq2seq tasks. Speci\\ufb01cally, we apply\\ntournament selection architecture search and warm start\\nit with the Transformer, considered to be the state-of-art\\nand widely-used, to evolve a better and more ef\\ufb01cient ar-\\nchitecture. To achieve this, we construct a search space\\nthat re\\ufb02ects the recent advances in feed-forward seq2seq\\nmodels and develop a method called Progressive Dynamic\\nHurdles (PDH) that allows us to perform our search directly\\non the computationally demanding WMT 2014 English-\\nGerman (En-De) translation task. Our search produces a\\nnew architecture \\u2013 called the Evolved Transformer (ET) \\u2013\\nwhich demonstrates consistent improvement over the orig-\\ninal Transformer on four well-established language tasks:\\nWMT 2014 English-German, WMT 2014 English-French\\n(En-Fr), WMT 2014 English-Czech (En-Cs) and the 1 Bil-\\nlion Word Language Model Benchmark (LM1B). At a big\\nmodel size, the Evolved Transformer establishes a new state-\\nof-the-art BLEU score of 29.8 on WMT\\u201914 En-De. It is\\nalso effective at smaller sizes, achieving the same quality\\nas the original \\u201dbig\\u201d Transformer with 37.6% less parame-\\nters and outperforming the Transformer by 0.7 BLEU at a\\nmobile-friendly model size of \\u00187M parameters.\\n2. Related Work\\nRNNs have long been used as the default option for ap-\\nplying neural networks to sequence modeling (SutskeverarXiv:1901.11117v4  [cs.LG]  17 May 2019\\nThe Evolved Transformer\\net al., 2014; Bahdanau et al., 2015), with LSTM (Hochre-\\niter & Schmidhuber, 1997) and GRU (Cho et al., 2014)\\narchitectures being the most popular. However, recent work\\nhas shown that RNNs are not necessary to build state-of-\\nthe-art sequence models. For example, many high perfor-\\nmance convolutional models have been designed, such as\\nWaveNet (Van Den Oord et al., 2016), Gated Convolution\\nNetworks (Dauphin et al., 2017), Conv Seq2Seq (Gehring\\net al., 2017) and the Dynamic Lightweight Convolution\\nmodel (Wu et al., 2019). Perhaps the most promising\\narchitecture in this direction is the Transformer architec-\\nture (Vaswani et al., 2017), which relies only on multi-head\\nattention to convey spatial information. In this work, we\\nuse both convolutions and attention in our search space to\\nleverage the strengths of both of these layer types.\\nThe recent advances in sequential feed-forward networks are\\nnot limited to architecture design. Various methods, such\\nas BERT (Devlin et al., 2018) and Radford et. al\\u2019s (2018)\\npre-training technique, have demonstrated how models such\\nas the Transformer can improve over RNN pre-training (Dai\\n& Le, 2015; Peters et al., 2018). For translation speci\\ufb01cally,\\nwork on scaling up batch size (Ott et al., 2018; Wu et al.,\\n2019), using relative position representations (Shaw et al.,\\n2018), and weighting multi-head attention (Ahmed et al.,\\n2017) have all pushed the state-of-the-art for WMT\\u201914 En-\\nDe and En-Fr. However, these methods are orthogonal to\\nthis work, as we are only concerned with improving the\\nneural network architecture itself, and not the techniques\\nused for improving overall performance.\\nThe \\ufb01eld of neural architecture search has also seen sig-\\nni\\ufb01cant recent progress. The best performing architecture\\nsearch methods are those that are computationally intensive\\n(Zoph & Le, 2017; Baker et al., 2016; Real et al., 2017; Xie\\n& Yuille, 2017; Zoph et al., 2018; Real et al., 2019). Other\\nmethods have been developed with speed in mind, such\\nas DARTS (Liu et al., 2018b), ENAS (Pham et al., 2018),\\nSMASH (Brock et al., 2018), and SNAS (Xie et al., 2019).\\nThese methods radically reduce the amount of time needed\\nto run each search by approximating the performance of\\neach candidate model, instead of investing resources to fully\\ntrain and evaluate each candidate separately. However, these\\nmethods also have several disadvantages that make them\\nhard to apply in our case: (1) It is hard to warm start these\\nmethods with the Transformer, which we found to be neces-\\nsary to yield strong results. (2) ENAS and DARTS require\\ntoo much memory at the model sizes we are searching for.\\n(3) The best architecture in the vision domain (e.g., Amoe-\\nbaNet(Real et al., 2019)) was discovered by evolutionary\\nNAS, not these ef\\ufb01cient methods, and we optimize for best\\narchitecture over best search ef\\ufb01ciency here.\\nZela et. al\\u2019s (2018) utilization of Hyperband (Li et al., 2017)\\nand PNAS\\u2019s (Liu et al., 2018a) incorporation of a surro-gate model are examples of approaches that try to both\\nincrease ef\\ufb01ciency via candidate performance estimation\\nand maximize search quality by training models to the end\\nwhen necessary. The Progressive Dynamic Hurdles (PDH)\\nmethod we introduce here is similar to these approaches\\nin that we train our best models to the end, but optimize\\nef\\ufb01ciency by discarding unpromising models early on. How-\\never, it is critically different from comparable algorithms\\nsuch as Hyperband and Successive Halving (Jamieson &\\nTalwalkar, 2016) in that it allows the evolution algorithm to\\ndynamically select new promising candidates as the search\\nprogresses; Hyperband and Successive Halving establish\\ntheir candidate pool a priori, which we demonstrate is inef-\\nfective in our large search space in Section 5.\\n3. Methods\\nWe employ evolution-based architecture search because it is\\nsimple and has been shown to be more ef\\ufb01cient than rein-\\nforcement learning when resources are limited (Real et al.,\\n2019). We use the same tournament selection (Goldberg &\\nDeb, 1991) algorithm as Real et al. (2019), with the aging\\nregularization omitted, and so encourage the reader to view\\ntheir in-depth description of the method. In the interest\\nof saving space, we will only give a brief overview of the\\nalgorithm here.\\nTournament selection evolutionary architecture search is\\nconducted by \\ufb01rst de\\ufb01ning a gene encoding that describes\\na neural network architecture; we describe our encoding in\\nthe following Search Space subsection. An initial popula-\\ntionis then created by randomly sampling from the space\\nof gene encodings to create individuals . These individu-\\nals, each corresponding to a neural architecture, are trained\\nand assigned \\ufb01tnesses , which in our case are the models\\u2019\\nnegative log perplexities on the WMT\\u201914 En-De validation\\nset. The population is then repeatedly sampled from to\\nproduce subpopulations , from which the individual with\\nthe highest \\ufb01tness is selected as a parent . Selected parents\\nhave their gene encodings mutated \\u2013 encoding \\ufb01elds ran-\\ndomly changed to different values \\u2013 to produce child models .\\nThese child models are then assigned a \\ufb01tness via training\\nand evaluating on the target task, as the initial population\\nwas. When this \\ufb01tness evaluation concludes, the population\\nis sampled from once again, and the individual in the sub-\\npopulation with the lowest \\ufb01tness is killed , meaning it is\\nremoved from the population. The newly evaluated child\\nmodel is then added to the population, taking the killed\\nindividual\\u2019s place. This process is repeated and results in a\\npopulation with high \\ufb01tness individuals, which in our case\\nrepresent well-performing architectures.\\nThe Evolved Transformer\\n3.1. Search Space\\nOur encoding search space is inspired by the NASNet search\\nspace (Zoph et al., 2018), but is altered to allow it to express\\narchitecture characteristics found in recent state-of-the-art\\nfeed-forward seq2seq networks. Crucially, we ensured that\\nthe search space can represent the Transformer, so that we\\ncould seed the initial population with it.\\nCell\\nleft hidden state left norm left layer to left output dim left activation combiner function  new hidden state  Block \\nright hidden state right norm right layer to right output dim right activation Left Branch Right Branch \\nFigure 1. Architecture composition from encoding. Each block\\nproduces a new hidden state that is added to the pool of hidden\\nstates subsequent blocks can select as branch inputs. Each encoder\\nhas 6 unique blocks per cell and each decoder has 8 unique blocks\\nper cell. Each cell is repeated number of cells times.\\nOur search space consists of two stackable cells, one for\\nthe model encoder and one for the decoder (see Figure 1).\\nEach cell contains NASNet-style blocks, which receive two\\nhidden state inputs and produce new hidden states as outputs;\\nthe encoder contains six blocks and the decoder contains\\neight blocks, so that the Transformer can be represented\\nexactly. The blocks perform separate transformations to\\neach input and then combine the transformation outputs\\ntogether to produce a single block output; we will refer to\\nthe transformations applied to each input as a branch . Our\\nsearch space contains \\ufb01ve branch-level search \\ufb01elds (input,\\nnormalization, layer, output dimension and activation), one\\nblock-level search \\ufb01eld (combiner function) and one cell-\\nlevel search \\ufb01eld (number of cells).\\nIn our search space, a child model\\u2019s genetic encoding is\\nexpressed as: [left input ,left normalization ,left layer ,left\\nrelative output dimension ,left activation ,right input ,right\\nnormalization ,right layer ,right relative output dimension ,\\nright activation ,combiner function ]\\u000214 +[number of cells ]\\n\\u00022, with the \\ufb01rst 6 blocks allocated to the encoder and the\\nlatter 8 allocated to the decoder. Given the vocabularies de-\\nscribed in the Supplementary Materials, this yields a search\\nspace of 7:30\\u000310115models, although we do shrink this to\\nsome degree by introducing constraints (see the Supplemen-\\ntary Materials for more details).3.2. Seeding the Search Space with Transformer\\nWhile previous neural architecture search works rely on\\nwell-formed hand crafted search spaces (Zoph et al., 2018),\\nwe intentionally leave our space minimally tuned, in a effort\\nto alleviate our manual burden and emphasize the role of the\\nautomated search method. To help navigate the large search\\nspace, we \\ufb01nd it easier to warm start the search process by\\nseeding our initial population with a known strong model,\\nin this case the Transformer. This anchors the search to a\\nknown good starting point and guarantees at least a single\\nstrong potential parent in the population as the generations\\nprogress. We offer empirical support for these claims in our\\nResults section.\\n3.3. Evolution with Progressive Dynamic Hurdles\\nThe evolution algorithm we employ is adapted from the\\ntournament selection evolutionary architecture search pro-\\nposed by Real et al. (2019), described above. Unlike Real\\net al. (2019) who conducted their search on CIFAR-10,\\nour search is conducted on a task that takes much longer to\\ntrain and evaluate on. Speci\\ufb01cally, to train a Transformer\\nto peak performance on WMT\\u201914 En-De requires \\u0018300K\\ntraining steps, or 10 hours, in the base size when using a\\nsingle Google TPU V .2 chip, as we do in our search. In\\ncontrast, Real et al. (2019) used the less resource-intensive\\nCIFAR-10 task (Krizhevsky & Hinton, 2009), which takes\\nabout two hours to train on, to assess their models during\\ntheir search, as it was a good proxy for ImageNet (Deng\\net al., 2009) performance (Zoph et al., 2018). However, in\\nour preliminary experimentation we could not \\ufb01nd a proxy\\ntask that gave adequate signal for how well each child model\\nwould perform on the full WMT\\u201914 En-De task; we investi-\\ngated using only a fraction of the data set and various forms\\nof aggressive early stopping.\\nTo address this problem we formulated a method to dynam-\\nically allocate resources to more promising architectures\\naccording to their \\ufb01tness. This method, which we refer to\\nasProgressive Dynamic Hurdles (PDH), allows models that\\nare consistently performing well to train for more steps. It\\nbegins as ordinary tournament selection evolutionary archi-\\ntecture search with early stopping, with each child model\\ntraining for a relatively small s0number of steps before\\nbeing evaluated for \\ufb01tness. However, after a predetermined\\nnumber of child models, m, have been evaluated, a hurdle ,\\nh0, is created by calculating the the mean \\ufb01tness of the\\ncurrent population. For the next mchild models produced,\\nmodels that achieve a \\ufb01tness greater than h0afters0train\\nsteps are granted an additional s1steps of training and then\\nare evaluated again to determine their \\ufb01nal \\ufb01tness. Once\\nanothermmodels have been considered this way, another\\nhurdle,h1, is constructed by calculating the mean \\ufb01tness\\nof all members of the current population that were trained\\nThe Evolved Transformer\\nfor the maximum number of steps. For the next mchild\\nmodels, training and evaluation continues in the same fash-\\nion, except models with \\ufb01tness greater than h1afters0+s1\\nsteps of training are granted an additional s2number of train\\nsteps, before being evaluated for their \\ufb01nal \\ufb01tness. This pro-\\ncess is repeated until a satisfactory number of maximum\\ntraining steps is reached. Algorithm 1 (Supplementary Ma-\\nterials) formalizes how the \\ufb01tness of an individual model\\nis calculated with hurdles and Algorithm 2 (Supplementary\\nMaterials) describes tournament selection augmented with\\nProgressive Dynamic Hurdles.\\nFigure 2. Evolution architecture search with hurdles. The y-\\naxis represents architecture \\ufb01tness and the x-axis represents the\\norder in which candidate models were created. The solid purple\\nand green lines represent the values of the \\ufb01rst and second hurdles,\\nrespectively. The dashed purple and green lines represent the\\npoints at which each of the corresponding hurdles were introduced.\\nPoints to the left of the purple dashed line were generated using\\nunaltered tournament selection. Between the purple and green\\ndashed lines, models with a \\ufb01tness above the solid purple line were\\ngranted additional train steps, forming a higher \\ufb01tness cluster. To\\nthe right of the green dashed line, models with a \\ufb01tness greater\\nthan the solid green line were granted a second round of additional\\ntrain steps.\\nAlthough different child models may train for different num-\\nbers of steps before being assigned their \\ufb01nal \\ufb01tness, this\\ndoes not make their \\ufb01tnesses incomparable. Tournament\\nselection evolution is only concerned with relative \\ufb01tness\\nrank when selecting which subpopulation members will be\\nkilled and which will become parents; the margin by which\\none candidate is better or worse than the other members\\nof the subpopulation does not matter. Assuming no model\\nover\\ufb01ts during its training, which is what we observed in\\nour experiments, and that its \\ufb01tness monotonically increases\\nwith respect to the number of train steps it is allocated, a\\ncomparison between two child models can be viewed as a\\ncomparison between their \\ufb01tnesses at the lower of the two\\u2019s\\ncumulative train steps. Since the model that was allocated\\nmore train steps performed, by de\\ufb01nition, above the \\ufb01tness\\nhurdle for the lower number of steps and the model that was\\nallocated less steps performed, by de\\ufb01nition, at or belowthat hurdle at the lower number of steps, it is guaranteed\\nthat the model with more train steps was better when it was\\nevaluated at the lower number of train steps.\\nThe bene\\ufb01t of altering the \\ufb01tness algorithm this way is that\\npoor performing child models will not consume as many\\nresources when their \\ufb01tness is being computed. As soon\\nas a candidate\\u2019s \\ufb01tness falls below a tolerable amount, its\\nevaluation immediately ends. This may also result in good\\ncandidates being labeled as bad models if they are only\\nstrong towards the latter part of training. However, the\\nresources saved as a result of discarding many bad models\\nimproves the overall quality of the search enough to justify\\npotentially also discarding some good ones; this is supported\\nempirically in our Results section.\\n4. Experiment Setup\\n4.1. Datasets\\nMachine Translation We use three different machine\\ntranslation datasets to perform our experiments, all of which\\nwere taken from their Tensor2Tensor implementations1. The\\n\\ufb01rst is WMT English-German, for which we mimic Vaswani\\net al.\\u2019s (2017) setup, using WMT\\u201918 En-De training data\\nwithout ParaCrawl (ParaCrawl, 2018), yielding 4.5 million\\nsentence pairs. In the same fashion, we use newstest2013 for\\ndevelopment and test on newstest2014. The second trans-\\nlation dataset is WMT En-Fr, for which we also replicate\\nVaswani et.al\\u2019s (2017) setup. We train on the 36 million sen-\\ntence pairs of WMT\\u201914 En-Fr, validate on newstest2013 and\\ntest on newstest2014. The \\ufb01nal translation dataset is WMT\\nEnglish-Czech (En-Cs). We used the WMT\\u201918 training\\ndataset, again without ParaCrawl, and used newstest2013\\nand newstest2014 as validation and test sets. For all tasks,\\ntokens were split using a shared source-target vocabulary of\\nabout 32K word-pieces (Wu et al., 2016).\\nAll datasets were generated using Tensor2Tensor\\u2019s \\u201cpacked\\u201d\\nscheme; sentences were shuf\\ufb02ed and concatenated together\\nwith padding to form uniform 256 length inputs and tar-\\ngets, with examples longer than 256 being discarded. This\\nyielded batch sizes of 4096 tokens per GPU or TPU chip;\\naccordingly, 16 TPU chip con\\ufb01gurations had \\u001866K tokens\\nper batch and 8 GPU chip con\\ufb01gurations had \\u001833K tokens\\nper batch.\\nLanguage Modeling For language modeling we used\\nthe 1 Billion Word Language Model Benchmark (LM1B)\\n(Chelba et al., 2013), also using its \\u201cpacked\\u201d Tensor2Tensor\\nimplementation. Again the tokens are split into a vocabulary\\nof approximately 32K word-pieces and the sentences are\\nshuf\\ufb02ed.\\n1https://github.com/tensor\\ufb02ow/tensor2tensor/tree/master/\\ntensor2tensor/data generators\\nThe Evolved Transformer\\n4.2. Training Details and Hyperparameters\\nMachine Translation All of our experiments used Ten-\\nsor2Tensor\\u2019s Transformer TPU hyperparameter settings2.\\nThese are nearly identical to those used by Vaswani et al.\\n(2017), but modi\\ufb01ed to use the memory-ef\\ufb01cient Adafactor\\n(Shazeer & Stern, 2018) optimizer. Aside from using the\\noptimizer itself, these hyperparameters also set the warmup\\nto a constant learning rate of 10\\u00002over 10K steps and\\nthen uses inverse-square-root learning-rate decay. For our\\nexperiments, we make only one change, which is to alter\\nthis decay so that it reaches 0 at the \\ufb01nal step of train-\\ning, which for our non-search experiments is uniformly\\n300K. We found that the our search candidate models, the\\nTransformer, and the Evolved Transformer all bene\\ufb01ted\\nfrom this and so experimented with using linear decay,\\nsingle-cycle cosine decay (Loshchilov & Hutter, 2017) and\\na modi\\ufb01ed inverse-square-root decay to 0 at 300K steps:\\nlr=step\\u00000:00303926\\u0000:962392 ; every decay was paired\\nwith the same constant 10\\u00002warmup. We used WMT En-\\nDe validation perplexity to gauge model performance and\\nfound that the Transformer preferred the modi\\ufb01ed inverse-\\nsquare-root decay. Therefore, this is what we used for both\\nall our Transformer trainings and the architecture searches\\nthemselves. The Evolved Transformer performed best with\\ncosine decay and so that is what we used for all of its train-\\nings. Besides this one difference, the hyperparameter set-\\ntings across models being compared are exactly the same.\\nBecause decaying to 0 resulted in only marginal weight\\nchanges towards the end of training, we did not use check-\\npoint averaging, except where noted.\\nPer-task there is one additional hyperparameter difference,\\nwhich is dropout rate. For ET and all search child models,\\ndropout was applied uniformly after each layer, approxi-\\nmating the Transformer\\u2019s more nuanced dropout scheme.\\nFor En-De and En-Cs, all \\u201cbig\\u201d and \\u201cdeep\\u201d sized models\\nwere given a higher dropout rate of 0.3, keeping in line\\nwith Vaswani et al. (2017), and all other models with an\\ninput embedding size of 768 are given a dropout rate of 0.2.\\nAside from this, hyperparameters are identical across all\\ntranslation tasks.\\nFor decoding we used the same beam decoding con\\ufb01gura-\\ntion used by Vaswani et al. (2017). That is a beam size of\\n4,length penalty (\\u000b) of 0.6, and maximum output length\\nof input length + 50. All BLEU is calculated using case-\\nsensitive tokenization3and for WMT\\u201914 En-De we also\\nuse the compound splitting that was used in Vaswani et al.\\n(2017).\\n2https://github.com/tensor\\ufb02ow/tensor2tensor/blob/master/\\ntensor2tensor/models/transformer.py\\n3https://github.com/moses-smt/mosesdecoder/blob/master/\\nscripts/generic/multi-bleu.perlLanguage Modeling Our language model training setup\\nis identical to our machine translation setup except we re-\\nmove label smoothing and lower the intra-attention dropout\\nrate to 0. This was taken from the Tensor2Tensor hyperpa-\\nrameters for LM1B2.\\n4.3. Search Con\\ufb01gurations\\nAll of the architecture searches we describe were run on\\nWMT\\u201914 En-De. They utilized the search space and tourna-\\nment selection evolution algorithm described in our Meth-\\nods section. Unless otherwise noted, each search used 200\\nworkers, which were equipped with a single Google TPU\\nV .2 chip for training and evaluation. We maintained a popu-\\nlation of size 100 with subpopulation sizes for both killing\\nand reproducing set to 30. Mutations were applied indepen-\\ndently per encoding \\ufb01eld at a rate of 2.5%. For \\ufb01tness we\\nused the negative log perplexity of the validation set instead\\nof BLEU because, as demonstrated in our Results section,\\nperplexity is more consistent and that reduced the noise of\\nour \\ufb01tness signal.\\n5. Results\\nIn this section, we will \\ufb01rst benchmark the performance of\\nour search method, Progressive Dynamic Hurdles, against\\nother evolutionary search methods (Real et al., 2017; 2019).\\nWe will then benchmark the Evolved Transformer, the result\\nof our search method, against the Transformer (Vaswani\\net al., 2017).\\n5.1. Ablation Study of Search Techniques\\nWe tested our evolution algorithm enhancements \\u2013 using\\nPDH and warm starting by seeding the initial population\\nwith the Transformer \\u2013 against control searches that did not\\nuse these techniques; without our enhancements, these con-\\ntrols function the same way as Real et. al\\u2019s (2019) searches,\\nwithout aging regularization. Each search we describe was\\nrun 3 times and the top model from each run was retrained\\non a single TPU V .2 chip for 300K steps. The performance\\nof the models after retraining is given in Table 1.\\nSEED MODELTRAIN\\nSTEPSNUM\\nMODELSTOPMODEL\\nPERPLEXITY\\nTRANSFORMER PDH 6000 4.50\\u00060.01\\nRANDOM PDH 6000 5.23 \\u00060.19\\nTRANSFORMER 15K 29714 4.57 \\u00060.01\\nTRANSFORMER 30K 14857 4.53 \\u00060.07\\nTRANSFORMER 180K 2477 4.58 \\u00060.05\\nTRANSFORMER 300K 1486 4.61 \\u00060.02\\nTable 1. Top model validation perplexity of various search se-\\ntups. Number of models were chosen to equalize resource con-\\nsumption.\\nThe Evolved Transformer\\nLayer Norm Gated Linear Unit : 512 Layer Norm Conv 1x1 : 2048 RELU \\nConv 3x1 : 256 Sep Conv 9x1 : 256 8 Head Self Attention : 512 Conv 1x1 : 2048 Conv 1x1 : 512 \\nLayer Norm Layer Norm Layer Norm \\nRELU RELU Evolved Transformer Encoder Block Transformer Encoder Block \\n8 Head Self Attention : 512 Conv 1x1 : 2048 Conv 1x1 : 512 \\nLayer Norm RELU \\n8 Head Self Attention : 512 Conv 1x1 : 2048 Conv 1x1 : 512 \\nLayer Norm Layer Norm RELU Layer Norm +\\n+\\n+\\n+Activation \\nNormalization \\nWide Convolution \\nAttention \\nNon-spatial Layer \\n+\\n+\\n+\\n++\\nEvolved Transformer Decoder Block Transformer Decoder Block \\n8 Head Attend to Encoder : 512 Conv 1x1 : 2048 Conv 1x1 : 512 \\nLayer Norm Layer Norm RELU Layer Norm \\n+\\nLayer Norm 16 Head Self Attention : 512 8 Head Attend to Encoder : 512 Layer Norm Sep Conv 11x1 : 1024 RELU \\nSep Conv 7x1 : 256 Layer Norm Sep Conv 7x1 : 512 \\n+\\n8 Head Self Attention : 512 \\nLayer Norm 8 Head Attend to Encoder : 512 Conv 1x1 : 2048 Conv 1x1 : 512 \\nLayer Norm Layer Norm RELU +\\n8 Head Self Attention : 512 \\n++\\n+\\n+Layer Norm 8 Head Attend to Encoder : 512 Conv 1x1 : 2048 Conv 1x1 : 512 \\nLayer Norm Layer Norm Swish +\\n8 Head Self Attention : 512 +\\n+\\n++\\nFigure 3. Transformer and Evolved Transformer architecture\\ncells. The four most notable aspects of the found architecture are\\nthe use of 1) wide depth-wise separable convolutions, 2) Gated\\nLinear Units (Dauphin et al., 2017), 3) branching structures and\\n4) swish activations (Ramachandran et al., 2017). Both the ET\\nencoder and decoder independently developed a branched lower\\nportion of wide convolutions. Also in both cases, the latter portion\\nis almost identical to the Transformer.\\nOur proposed search (Table 1 row 1), which used both PDH\\nand Transformer seeding, was run \\ufb01rst, with hurdles created\\nevery 1K models ( m= 1000 ) and six 30K train step (1\\nhour) increments ( s=<30;30;30;30;30;30>). To test\\nthe effectiveness of seeding with the Transformer, we ran\\nan identical search that was instead seeded with random\\nvalid encodings (Table 1 row 2). To test the effectiveness\\nof PDH, we ran four controls (Table 1 rows 3-6) that each\\nused a \\ufb01xed number of train steps for each child model\\ninstead of hurdles (Table 1 column 2). For these we used\\nthe step increments (30K), the maximum number of steps\\nour proposed search ultimately reaches (180K), the totalnumber of steps each top model receives when fully trained\\nto gauge its \\ufb01nal performance (300K), and half the step\\nincrements (15K), recognizing the gains from evaluating a\\nlarger number of models in the 30K steps control case. To\\ndetermine the number of child models each of these searches\\nwould be able to train, we selected the value that would\\nmake the total amount of resources used by each control\\nsearch equal to the maximum amount of resources used for\\nour proposed searches, which require various amounts of\\nresources depending on how many models fail to overcome\\nhurdles. In the three trials we ran, our proposed search\\u2019s\\ntotal number of train steps used was 422M \\u000621M, with\\na maximum of 446M. Thus the number of child models\\nallotted for each non-PDH control search was set so that\\nthe total number of child model train steps used would be\\n446M.\\nAs demonstrated in Table 1, the search we propose, with\\nPDH and Transformer seeding, has the best performance on\\naverage. It also is the most consistent, having the lowest\\nstandard deviation. Of all the searches conducted, only a\\nsingle control run \\u2013 \\u201c30K no hurdles\\u201d (Table 1 row 3) \\u2013\\nproduced a model that was better than any of our proposed\\nsearch\\u2019s best models. At the same time, the \\u201c30K no hur-\\ndles\\u201d setup also produced models that were signi\\ufb01cantly\\nworse, which explains its high standard deviation. This\\nphenomenon was a chief motivator for our developing this\\nmethod. Although aggressive early stopping has the poten-\\ntial to produce strong models for cheap, searches that utilize\\nit can also venture into modalities in which top \\ufb01tness child\\nmodels are only strong early on; without running models for\\nlonger, whether or not this is happening cannot be detected.\\nFor example, the 15K search performed worse than the 30K\\nsetting, despite evaluating twice as many models. Although\\nthe 180K and 300K searches did have insight into long term\\nperformance, it was in a resource-inef\\ufb01cient manner that\\nhurt these searches by limiting the number of generations\\nthey produced; for the \\u201c180K no hurdles\\u201d run to train as\\nmany models as PDH would require 1.08B train steps, over\\ndouble what PDH used in our worst case.\\nSearching with random seeding also proved to be ineffective,\\nperforming considerably worse than every other con\\ufb01gu-\\nration. Of the \\ufb01ve searches run, random seeding was the\\nonly one that had a top model perplexity higher than the\\nTransformer, which is 4.75 \\u00060.01 in the same setup.\\n5.2. Main Search.\\nAfter con\\ufb01rming the effectiveness of our search procedure,\\nwe launched a larger scale version of our search using 270\\nworkers. We trained 5K models per hurdle ( m= 5000 ) and\\nused larger step increments to get a closer approximation\\nto 300K step performance: s=<60;60;120>. The setup\\nwas the same as the Search Techniques experiments, except\\nafter 11K models we lowered the mutation rate to 0.01 and\\nThe Evolved Transformer\\nintroduced the NONE value to the normalization mutation\\nvocabulary.\\nThe search ran for 15K child models, requiring a total of\\n979M train steps. Over 13K models did not make it past the\\n\\ufb01rst hurdle, drastically reducing the resources required to\\nview the 240 thousandth train step for top models, which\\nwould have cost 3.6B train steps for the same number of\\nmodels without hurdles. After the search concluded, we then\\nselected the top 20 models and trained them for the full 300K\\nsteps, each on a single TPU V .2 chip. The model that ended\\nwith the best perplexity is what we refer to as the Evolved\\nTransformer (ET). Figure 3 shows the ET architecture. The\\nmost notable aspect of the Evolved Transformer is the use of\\nwide depth-wise separable convolutions in the lower layers\\nof the encoder and decoder blocks. The use of depth-wise\\nconvolution and self-attention was previously explored in\\nQANet (Yu et al., 2018), however the overall architectures\\nof the Evolved Transformer and QANet are different in\\nmany ways: e.g., QANet has smaller kernel sizes and no\\nbranching structures. The performance and analysis of the\\nEvolved Transformer will be shown in the next section.\\n5.3. The Evolved Transformer: Performance and\\nAnalysis\\nTo test the effectiveness of the found architecture \\u2013 the\\nEvolved Transformer \\u2013 we compared it to the Transformer\\nin its Tensor2Tensor training regime on WMT\\u201914 En-De.\\nTable 3 shows the results of these experiments run on the\\nsame 8 NVIDIA P100 hardware setup that was used by\\nVaswani et al. (2017). Observing ET\\u2019s improved perfor-\\nmance at parameter-comparable \\u201cbase\\u201d and \\u201cbig\\u201d sizes, we\\nwere also interested in understanding how small ET could\\nbe shrunk while still achieving the same performance as the\\nTransformer. To create a spectrum of model sizes for each\\narchitecture, we selected different input embedding sizes\\nand shrank or grew the rest of the model embedding sizes\\nwith the same proportions. Aside from embedding depths,\\nthese models are identical at all sizes, except the \\u201cbig\\u201d 1024\\ninput embedding size, for which all 8 head attention layers\\nare upgraded to 16 head attention layers, as was done in\\nVaswani et al. (2017).\\nET demonstrates stronger performance than the Transformer\\nat all sizes, with the largest difference of 0.7 BLEU at the\\nsmallest, mobile-friendly, size of \\u00187M parameters. Per-\\nformance on par with the \\u201cbase\\u201d Transformer was reached\\nwhen ET used just 78.4% of its parameters and performance\\nof the \\u201cbig\\u201d Transformer was exceeded by the ET model\\nthat used 37.6% less parameters. Figure 4 shows the FLOPS\\nvs. BLEU performance of both architectures.\\nTo test if ET\\u2019s strong performance generalizes, we also\\ncompared it to the Transformer on an additional three well-\\nestablished language tasks: WMT\\u201914 En-Fr, WMT\\u201914 En-\\nFigure 4. Performance comparison of the Evolved Trans-\\nformer against the Transformer across number of parame-\\nters.\\nCs, and LM1B.4Upgrading to 16 TPU V .2 chips, we dou-\\nbled the number of synchronous workers for these experi-\\nments, pushing both models to their higher potential (Ott\\net al., 2018). We ran each con\\ufb01guration 3 times, except\\nWMT En-De, which we ran 6 times; this was a matter of\\nresource availability and we gave priority to the task we\\nsearched on. As shown in Table 2, ET performs at least one\\nstandard deviation above the Transformer in each of these\\ntasks. Note that the Transformer mean BLEU scores in all\\nof our experiments for WMT\\u201914 En-Fr and WMT\\u201914 En-De\\nare higher than those originally reported by Vaswani et al.\\n(2017).\\nAs can be seen in Tables 3 and 2, the Evolved Transformer is\\nmuch more effective than the Transformer at smaller model\\nsizes. At the \\u201cbig\\u201d model size, its BLEU performance sat-\\nurates and the gap between the Evolved Transformer and\\nthe Transformer becomes smaller. One explanation for this\\nbehavior is that over\\ufb01tting starts to occur at big model sizes,\\nbut we expect that data augmentation (Ott et al., 2018) or\\nhyperparameter tuning could improve performance. For ex-\\nample, we found that simply increasing the embedding size\\nwas not the best way to grow ET from the \\u201cbase\\u201d size we\\nsearched over to a larger size. Depth should also be tuned\\nin conjunction with embedding size, when controlling for\\nnumber of parameters. For both the Transformer and ET,\\nwe tried four additional embedding sizes, [512, 640, 768,\\n896], adjusting the depth accordingly so that all resulting\\nmodels had a similar number of parameters. Using valida-\\ntion BLEU to determine the best con\\ufb01guration, we found\\nthat ET performed best with an embedding depth of 640,\\nincreasing its number of encoder cells from 3 to 9 and its\\nnumber of decoder cells from 4 to 10. The Transformer also\\nbene\\ufb01ted from additional depth, although not to the same\\ndegree, achieving maximum performance at the 768 embed-\\nding size, with 6 encoder cells and 6 decoder cells. These\\nresults are included in Table 2, labeled as \\u201cDeep\\u201d size.\\n4For LM1B, we only use the decoder architecture, with attend\\nto encoder layers removed.\\nThe Evolved Transformer\\nTASK SIZETRAN\\nPARAMSET\\nPARAMSTRAN PERP ET P ERP TRAN BLEU ET BLEU\\nWMT\\u201914 E N-DEBASE 61.1M 64.1M 4.24 \\u00060.03 4.03\\u00060.02 28.2 \\u00060.2 28.4\\u00060.2\\nWMT\\u201914 E N-DE BIG 210.4M 221.7M 3.87 \\u00060.02 3.77\\u00060.02 29.1 \\u00060.1 29.3\\u00060.1\\nWMT\\u201914 E N-DEDEEP 224.0M 218.1M 3.86 \\u00060.02 3.69\\u00060.01 29.2 \\u00060.1 29.5\\u00060.1\\nWMT\\u201914 E N-FR BASE 60.8 63.8M 3.61 \\u00060.01 3.42\\u00060.01 40.0 \\u00060.1 40.6\\u00060.1\\nWMT\\u201914 E N-FR BIG 209.8M 221.2M 3.26 \\u00060.01 3.13\\u00060.01 41.2 \\u00060.1 41.3\\u00060.1\\nWMT\\u201914 E N-CS BASE 59.8M 62.7M 4.98 \\u00060.04 4.42\\u00060.01 27.0 \\u00060.1 27.6\\u00060.2\\nWMT\\u201914 E N-CS BIG 207.6M 218.9M 4.43 \\u00060.01 4.38\\u00060.03 28.1 \\u00060.1 28.2\\u00060.1\\nLM1B B IG 141.1M 151.8M 30.44 \\u00060.04 28.60\\u00060.03 - -\\nTable 2. Comparison between the Transformer and ET trained on 16 TPU V .2 chips. For Translation, perplexity was calculated on\\nthe validation set and BLEU was calculated on the test set. For LM1B, perplexity was calculated on the test set. ET shows consistent\\nimprovement by at least one standard deviation on all tasks. It excels at the base size the search was conducted in, with an improvement of\\n0.6 BLEU in both En-Fr and En-Cs.\\nModelEmbedding\\nSizeParameters Perplexity BLEU \\u0001BLEU\\nTransformer 128 7.0M 8.62 \\u00060.03 21.3 \\u00060.1 -\\nET 128 7.2M 7.62\\u00060.02 22.0\\u00060.1 + 0.7\\nTransformer 432 45.8M 4.65 \\u00060.01 27.3 \\u00060.1 -\\nET 432 47.9M 4.36\\u00060.01 27.7\\u00060.1 + 0.4\\nTransformer 512 61.1M 4.46 \\u00060.01 27.7 \\u00060.1 -\\nET 512 64.1M 4.22\\u00060.01 28.2\\u00060.1 + 0.5\\nTransformer 768 124.8M 4.18 \\u00060.01 28.5 \\u00060.1 -\\nET 768 131.2M 4.00\\u00060.01 28.9\\u00060.1 + 0.4\\nTransformer 1024 210.4M 4.05 \\u00060.01 28.8 \\u00060.2 -\\nET 1024 221.7M 3.94\\u00060.01 29.0\\u00060.1 + 0.2\\nTable 3. WMT\\u201914 En-De comparison on 8 NVIDIA P100 GPUs. Each model was trained 10 to 15 times, depending on resource\\navailability. Perplexity is calculated on the validation set and BLEU is calculated on the test set.\\nModel Params BLEUSacreBLEU\\n(Post, 2018)\\nGehring et al. (2017) 216M 25.2 -\\nVaswani et al. (2017) 213M 28.4 -\\nAhmed et al. (2017) 213M 28.9 -\\nChen et al. (2018) 379M 28.5 -\\nShaw et al. (2018) 213M 29.2 -\\nOtt et al. (2018) 210M 29.3 28.6\\nWu et al. (2019) 213M 29.7 -\\nEvolved Transformer 218M 29.8 29.2\\nTable 4. Model comparison on WMT\\u201914 En-De.\\nTo compare with other previous results, we trained the ET\\nDeep model three times in our TPU setup on WMT\\u201914\\nEn-De, selected the best run according to validation BLEU\\nand did a single decoding on the test set. We also copied\\nprevious state-of-the-art result setups by averaging the last\\n20 model checkpoints from training and decoding with a\\nbeam width of 5 (Vaswani et al., 2017; Ott et al., 2018; Wu\\net al., 2019). As a result, the Evolved Transformer achieved\\na new state-of-the-art BLEU score of 29.8 (Table 4).6. Conclusion\\nWe presented the \\ufb01rst neural architecture search conducted\\nto \\ufb01nd improved feed-forward sequence models. We \\ufb01rst\\nconstructed a large search space inspired by recent advances\\nin seq2seq models and used it to search directly on the\\ncomputationally intensive WMT En-De translation task. To\\nmitigate the size of our space and the cost of training child\\nmodels, we proposed using both our Progressive Dynamic\\nHurdles method and warm starting, seeding our initial pop-\\nulation with a known strong model, the Transformer.\\nWhen run at scale, our search found the Evolved Trans-\\nformer. In a side by side comparison against the Trans-\\nformer in an identical training regime, the Evolved Trans-\\nformer showed consistent stronger performance on both\\ntranslation and language modeling. On the task we searched\\nover, WMT\\u201914 En-De, the Evolved Transformer established\\na new state-of-the-art of 29.8 BLEU. It also proved to be\\nef\\ufb01cient at smaller sizes, achieving the same quality as the\\noriginal \\u201dbig\\u201d Transformer with 37.6% less parameters and\\noutperforming the Transformer by 0.7 BLEU at a mobile-\\nfriendly model size of \\u00187M parameters.\\nThe Evolved Transformer\\nAcknowledgements\\nWe would like to thank Ashish Vaswani, Jakob Uszkoreit,\\nNiki Parmar, Noam Shazeer, Lukasz Kaiser and Ryan Sepa-\\nssi for their help with Tensor2Tensor and for sharing their\\nunderstanding of the Transformer. We are also grateful\\nto David Dohan, Esteban Real, Yanping Huang, Alok Ag-\\ngarwal, Vijay Vasudevan, and Chris Ying for lending their\\nexpertise in architecture search and evolution.\\nReferences\\nAhmed, K., Keskar, N. S., and Socher, R. Weighted\\ntransformer network for machine translation. CoRR ,\\nabs/1711.02132, 2017. URL http://arxiv.org/\\nabs/1711.02132 .\\nBa, L. J., Kiros, R., and Hinton, G. E. Layer normalization.\\nCoRR , abs/1607.06450, 2016. URL http://arxiv.\\norg/abs/1607.06450 .\\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\\ntranslation by jointly learning to align and translate. In\\nInternational Conference on Learning Representations ,\\n2015.\\nBaker, B., Gupta, O., Naik, N., and Raskar, R. Designing\\nneural network architectures using reinforcement learn-\\ning. In International Conference on Learning Represen-\\ntations , 2016.\\nBrock, A., Lim, T., Ritchie, J., and Weston, N. SMASH:\\nOne-shot model architecture search through hypernet-\\nworks. In International Conference on Learning Rep-\\nresentations , 2018. URL https://openreview.\\nnet/forum?id=rydeCEhs- .\\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T.,\\nand Koehn, P. One billion word benchmark for measur-\\ning progress in statistical language modeling. CoRR ,\\nabs/1312.3005, 2013. URL http://arxiv.org/\\nabs/1312.3005 .\\nChen, M. X., Firat, O., Bapna, A., Johnson, M., Macherey,\\nW., Foster, G., Jones, L., Parmar, N., Schuster, M., Chen,\\nZ., Wu, Y ., and Hughes, M. The best of both worlds:\\nCombining recent advances in neural machine translation.\\nCoRR , abs/1804.09849, 2018. URL http://arxiv.\\norg/abs/1804.09849 .\\nCho, K., Merrienboer, B. V ., Bahdanau, D., and Bengio,\\nY . On the properties of neural machine translation: En-\\ncoderdecoder approaches. In Eighth Workshop on Syntax,\\nSemantics and Structure in Statistical Translation , 2014.\\nDai, A. M. and Le, Q. V . Semi-supervised sequence learning.\\nInAdvances in Neural Information Processing Systems ,\\npp. 3079\\u20133087, 2015.Dauphin, Y . N., Fan, A., Auli, M., and Grangier, D. Lan-\\nguage modeling with gated convolutional networks. In\\nInternational Conference on Machine Learning , 2017.\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\\nL. Imagenet: A large-scale hierarchical image database.\\nIEEE Conference on Computer Vision and Pattern Recog-\\nnition , pp. 248\\u2013255, 2009.\\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:\\npre-training of deep bidirectional transformers for lan-\\nguage understanding. volume abs/1810.04805, 2018.\\nURL http://arxiv.org/abs/1810.04805 .\\nElfwing, S., Uchibe, E., and Doya, K. Sigmoid-weighted\\nlinear units for neural network function approximation in\\nreinforcement learning. Neural Networks , 2018.\\nGehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin,\\nY . N. Convolutional sequence to sequence learning. In\\nInternational Conference on Machine Learning , 2017.\\nGoldberg, D. E. and Deb, K. A comparative analysis of\\nselection schemes used in genetic algorithms. In Founda-\\ntions of Genetic Algorithms , 1991.\\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\\nInNeural Computation , pp. 1735\\u20131780. Massachusetts\\nInstitute of Technology, 1997.\\nJamieson, K. G. and Talwalkar, A. S. Non-stochastic best\\narm identi\\ufb01cation and hyperparameter optimization. In\\nAISTATS , 2016.\\nKrizhevsky, A. and Hinton, G. Learning multiple layers of\\nfeatures from tiny images. 2009.\\nLi, L., Jamieson, K. G., DeSalvo, G., Rostamizadeh, A.,\\nand Talwalkar, A. S. Hyperband: a novel bandit-based\\napproach to hyperparameter optimization. In Journal of\\nMachine Learning Research , 2017.\\nLiu, C., Zoph, B., Neumann, M., Shlens, J., Hua, W., Li,\\nL.-J., Fei-Fei, L., Yuille, A. L., Huang, J., and Murphy,\\nK. Progressive neural architecture search. In European\\nConference on Computer Vision , 2018a.\\nLiu, H., Simonyan, K., and Yang, Y . Darts: Differentiable\\narchitecture search. In DARTS: differentiable architecture\\nsearch , 2018b.\\nLoshchilov, I. and Hutter, F. Sgdr: Stochastic gradient\\ndescent with warm restarts. In International Conference\\non Learning Representations , 2017.\\nMaas, A. L., Hannun, A. Y ., and Ng, A. Y . Recti\\ufb01er non-\\nlinearities improve neural network acoustic models. In\\nInternational Conference on Machine Learning , 2013.\\nThe Evolved Transformer\\nOtt, M., Edunov, S., Grangier, D., and Auli, M. Scaling\\nneural machine translation. In Workshop on Machine\\nTranslation, Empirical Methods in Natural Language\\nProcessing , 2018.\\nParaCrawl, 2018. URL http://paracrawl.eu/\\ndownload.html .\\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\\nword representations. In Proc. of NAACL , 2018.\\nPham, H., Guan, M. Y ., Zoph, B., Le, Q. V ., and Dean, J.\\nEf\\ufb01cient neural architecture search via parameter sharing.\\nInInternational Conference on Machine Learning , 2018.\\nPost, M. A call for clarity in reporting BLEU scores. CoRR ,\\nabs/1804.08771, 2018. URL http://arxiv.org/\\nabs/1804.08771 .\\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\\nI. Improving language understanding by generative pre-\\ntraining, 2018. URL https://blog.openai.com/\\nlanguage-unsupervised/ .\\nRamachandran, P., Zoph, B., and Le, Q. V . Searching for\\nactivation functions. CoRR , abs/1710.05941, 2017. URL\\nhttp://arxiv.org/abs/1710.05941 .\\nReal, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y . L.,\\nTan, J., Le, Q., and Kurakin, A. Large-scale evolution\\nof image classi\\ufb01ers. In International Conference on Ma-\\nchine Learning , 2017.\\nReal, E., Aggarawal, A., Huang, Y ., and Le, Q. V . Regular-\\nized evolution for image classi\\ufb01er architecture search. In\\nInternational Conference on Learning Representations ,\\n2019.\\nShaw, P., Uszkoreit, J., and Vaswani, A. Self-\\nattention with relative position representations. volume\\nabs/1803.02155, 2018. URL http://arxiv.org/\\nabs/1803.02155 .\\nShazeer, N. and Stern, M. Adafactor: adaptive\\nlearning rates with sublinear memory cost. CoRR ,\\nabs/1804.04235, 2018. URL http://arxiv.org/\\nabs/1804.04235 .\\nSutskever, I., Vinyals, O., and Le, Q. V . Sequence to se-\\nquence learning with neural networks. In Advances in\\nNeural Information Processing Systems , pp. 31043112,\\n2014.\\nVan Den Oord, A., Dieleman, S., Zen, H., Simonyan, K.,\\nVinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and\\nKavukcuoglu, K. Wavenet: A generative model for raw\\naudio. CoRR abs/1609.03499 , 2016.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-\\ntion is all you need. In Neural Information Processing\\nSystems , 2017.\\nVaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez,\\nA. N., Gouws, S., Jones, L., ukasz Kaiser, Kalchbrenner,\\nN., Parmar, N., Sepassi, R., Shazeer, N., and Uszkoreit, J.\\nTensor2tensor for neural machine translation. In Comput-\\ning Research Repository , 2018. abs/1803.07416 .\\nWu, F., Fan, A., Baevski, A., Dauphin, Y ., and Auli, M. Pay\\nless attention with lightweight and dynamic convolutions.\\nInInternational Conference on Learning Representations ,\\n2019.\\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M.,\\nMacherey, W., Krikun, M., Cao, Y ., Gao, Q., Macherey,\\nK., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser,\\nL., Gouws, S., Kato, Y ., Kudo, T., Kazawa, H., Stevens,\\nK., Kurian, G., Patil, N., Wang, W., Young, C., Smith,\\nJ., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G. S.,\\nHughes, M., and Dean, J. Google\\u2019s neural machine trans-\\nlation system: Bridging the gap between human and ma-\\nchine translation. CoRR , abs/1609.08144, 2016.\\nXie, L. and Yuille, A. L. Genetic cnn. 2017 IEEE In-\\nternational Conference on Computer Vision (ICCV) , pp.\\n1388\\u20131397, 2017.\\nXie, S., Zheng, H., Liu, C., and Lin, L. SNAS: stochastic\\nneural architecture search. In International Conference\\non Learning Representations , 2019.\\nYu, A. W., Dohan, D., Luong, T., Zhao, R., Chen, K.,\\nNorouzi, M., and Le, Q. V . Fast and accurate reading com-\\nprehension by combining self-attention and convolution.\\nInInternational Conference on Learning Representations ,\\n2018.\\nZela, A., Klein, A., Falkner, S., and Hutter, F. Towards\\nautomated deep learning: ef\\ufb01cient joint neural architec-\\nture and hyperparameter search. In Workshop on AutoML,\\nInternational Conference on Machine Learning , 2018.\\nZoph, B. and Le, Q. V . Neural architecture search with\\nreinforcement learning. In International Conference on\\nLearning Representations , 2017.\\nZoph, B., Vasudevan, V ., Shlens, J., and Le, Q. V . Learning\\ntransferable architectures for scalable image recognition.\\nInConference on Computer Vision and Pattern Recogni-\\ntion, 2018.\\nThe Evolved Transformer\\nA. Search Algorithms\\nIn the following, we describe the algorithm that we use to\\ncalculate child model \\ufb01tness with hurdles (Algorithm 1)\\nand evolution architecture search with Progressive Dynamic\\nHurdles (Algorithm 2).\\nAlgorithm 1 Calculate Model Fitness with Hurdles\\ninputs:\\nmodel : the child model\\ns: vector of train step increments\\nh: queue of hurdles\\nappend1toh\\nTRAIN NSTEPS(model ,s0)\\nfitness EV ALUATE( model )\\ni 0\\nhurdle hi\\nwhilefitness>hurdle do\\ni i+ 1\\nTRAIN NSTEPS(model ,si)\\nfitness EV ALUATE( model )\\nhurdle hi\\nend while\\nreturnfitness\\nAlgorithm 1 Calculating \\ufb01tness with hurdles takes as ar-\\nguments a child model, a vector of train step increments ( s)\\nand a queue of hurdles( h). The child model is the candidate\\nmodel in our neural architecture search. The vector of step\\nincrements describes the number of steps between each hur-\\ndle; its length must be greater than 0. The queue of hurdles\\ndescribes what hurdles have already been established; its\\nlength must be in [0, length (s)).\\nThe algorithm starts by \\ufb01rst training the child model a \\ufb01xed\\nnumber ofs0steps and evaluating on the validation set to\\nproduce a \\ufb01tness, as is done in Real et al. (2019). After this\\nbaseline \\ufb01tness is established, the hurdles ( h) are compared\\nagainst to determine if training should continue. Each hi\\ndenotes the \\ufb01tness a child model must have afterPi\\nj=0sj\\ntrain steps to continue training. Each time a hurdle hiis\\npassed, the model is trained an additional si+1steps. If the\\nmodel\\u2019s \\ufb01tness ever falls below the hurdle corresponding\\nto the number of steps it was trained for, training ends\\nimmediately and the current \\ufb01tness is returned. If the model\\nnever falls below a hurdle and all hurdles have been passed,\\nthe child model receives one \\ufb01nal training of slength (h)steps\\nbefore \\ufb01tness is returned; this is expressed in Algorithm 1\\nwith1being appended to the end of h.Algorithm 2 Evolution Architecture Search with PDH\\ninputs:\\ns: vector of train step increments\\nm: number of child models per hurdle\\nh empty queue\\ni 0\\npopulation INITIAL POPULATION()\\nwhilei<LENGTH(s) - 1 do\\npopulation EVOL NMODELS(population ,\\nm,s,h)\\nhurdle MEAN FITNESS OFMAX(population )\\nappendhurdle toh\\nend while\\npopulation EVOL NMODELS(population ,\\nm,s,h)\\nreturnpopulation\\nAlgorithm 2 Evolution architecture search with PDH\\ntakes as arguments a vector of train step increments ( s)\\nand a number of child models per hurdle ( m). It begins\\nas Real et al.\\u2019s (2019) evolution architecture search with a\\n\\ufb01xed number of child model train steps, s0. However, after\\nmchild models have been produced, a hurdle is created\\nby taking the mean \\ufb01tness of the current population and\\nit is added to the hurdle queue, h. Algorithm 1 is used to\\ncompute each child model\\u2019s \\ufb01tness and so if they overcome\\nthe new hurdle they will receive more train steps. This pro-\\ncess is continued, with new hurdles being created using the\\nmean \\ufb01tness of all models that have trained the maximum\\nnumber of steps and hgrowing accordingly. The process\\nterminates when length (s)\\u00001hurdles have been created\\nand evolution is run for one last round of mmodels, using\\nall created hurdles.\\nB. Search Space Information\\nIn our search space, a child model\\u2019s genetic encoding is\\nexpressed as: [left input ,left normalization ,left layer ,left\\nrelative output dimension ,left activation ,right input ,right\\nnormalization ,right layer ,right relative output dimension ,\\nright activation ,combiner function ]\\u000214 +[number of cells ]\\n\\u00022, with the \\ufb01rst 6 blocks allocated to the encoder and the\\nlatter 8 allocated to the decoder. In the following, we will\\ndescribe each of the components.\\nInput. The \\ufb01rst branch-level search \\ufb01eld is input . This\\nspeci\\ufb01es what hidden state in the cell will be fed as input\\nto the branch. For each ithblock, the input vocabulary of\\nits branches is [0;i), where thejthhidden state corresponds\\nto thejthblock output and the 0thhidden state is the cell\\nThe Evolved Transformer\\ninput.\\nNormalization. The second branch-level search \\ufb01eld is\\nnormalization , which is applied to each input before the\\nlayer transformation is applied. The normalization vocabu-\\nlary is [ LAYER NORMALIZATION (Ba et al., 2016), NONE ].\\nLayers. The third branch-level search \\ufb01eld is layer , which\\nis the neural network layer applied after the normalization.\\nIt\\u2019s vocabulary is:\\n\\u000fSTANDARD CONV wx1: forw2f1;3g\\n\\u000fDEPTHWISE SEPARABLE CONV wx1: forw2\\nf3;5;7;9;11g\\n\\u000fLIGHTWEIGHT CONV wx1r: forw2f3;5;7;15g,\\nr2f1;4;16g(Wu et al., 2019). ris the reduction\\nfactor, equivalent to d=H described in Wu et al. (2019).\\n\\u000fhHEAD ATTENTION : forh2f4;8;16g\\n\\u000fGATED LINEAR UNIT (Dauphin et al., 2017)\\n\\u000fATTEND TO ENCODER : (Only available to decoder)\\n\\u000fIDENTITY : No transformation applied to input\\n\\u000fDEAD BRANCH : No output\\nFor decoder convolution layers the inputs are shifted by\\n(w\\u00001)=2so that positions cannot \\u201csee\\u201d later predictions.\\nRelative Output Dimension. The fourth branch-level\\nsearch \\ufb01eld is relative output dimension , which describes\\nthe output dimension of the corresponding layer. The Trans-\\nformer is composed mostly of layers that project to the\\noriginal input embedding depth (512 in the \\u201cbase\\u201d con\\ufb01gu-\\nration), but also contains 1x1 convolutions that project up\\nto a dimension of 4 times that depth (2048 in the \\u201cbase\\u201d\\ncon\\ufb01guration). The relative output dimension search \\ufb01eld\\naccounts for this variable output depth. It\\u2019s vocabulary con-\\nsists of 10 relative output size options: [1;10].\\nHere \\u201crelative\\u201d refers to the fact that for every layer iandj,\\neach of their absolute output dimensions, a, and relative out-\\nput dimensions, d, will obey the ratio: ai=aj=di=dj. We\\ndetermine the absolute output dimensions for each model\\nby \\ufb01nding a scaling factor, s, such that for every layer i,\\nai=di\\u0003sand the resulting model has an appropriate num-\\nber of parameters; at the end of this section, we describe\\nour constraints on number of model parameters. There may\\nbe multiple values of sfor any one model that satisfy this\\nconstraint, and so for our experiments we simply perform\\na binary search and use the \\ufb01rst valid value found. If no\\nvalid value is found, we reject the child model encoding as\\ninvalid and produce a new one in its stead.\\nWe chose a vocabulary of relative sizes instead of absolute\\nsizes because we only allow models within a \\ufb01xed param-\\neter range, as described later in this section (Constraints).\\nUsing relative sizes allows us to increase the number of con-\\n\\ufb01gurations that represent valid models in our search space,because we can dynamically shrink or grow a model to make\\nit \\ufb01t within the parameter bounds. We found that using ab-\\nsolute values, such as [256;512;1024;2048] , increases the\\nnumber of rejected models and thereby decreases the possi-\\nble models that can be expressed.\\nThis relative output dimensions \\ufb01eld is ignored for both the\\nIDENTITY and DEAD BRANCH layers.\\nActivations. The \\ufb01nal branch-level search \\ufb01eld is acti-\\nvation , which is the non-linearity applied on each branch\\nafter the neural network layer. The activation vocabulary is\\nfSWISH (Ramachandran et al., 2017; Elfwing et al., 2018),\\nRELU ,LEAKY RELU (MAAS ET AL ., 2013), NONEg.\\nCombiner Functions. The block-level search \\ufb01eld, com-\\nbiner function , describes how the left and right layer\\nbranches are combined together. Its vocabulary is\\nfADDITION ,CONCATENATION ,MULTIPLICATIONg. For\\nMULTIPLICATION and ADDITION , if the right and left\\nbranch outputs have differing embedding depths, then the\\nsmaller of the two is padded so that the dimensionality\\nmatches. For ADDITION the padding is 0\\u2019s; for MULTIPLI -\\nCATION the padding is 1\\u2019s.\\nNumber of Cells. The cell-level search \\ufb01eld is number of\\ncells and it describes the number of times the cell is repeated.\\nIts vocabulary is [1,6].\\nComposition. Each child model is de\\ufb01ned by two cells,\\none for the encoder and one for the decoder. The encoder\\ncell contains 6 blocks and the decoder cell contains 8 blocks.\\nEach block contains two branches, each of which takes a\\nprevious hidden layer as input, and then applies its normal-\\nization, layer (with speci\\ufb01ed relative output dimensions)\\nand activation to it. The two branches are then joined with\\nthe combiner function. Any unused hidden states are auto-\\nmatically added to the \\ufb01nal block output via addition. Both\\nthe encoder and decoder cells de\\ufb01ned this way are repeated\\ntheir corresponding number of cells times and connected\\nto the input and output embedding portions of the network\\nto produce the \\ufb01nal model; we use the same embedding\\nscheme described by Vaswani et al. (2017) for all models.\\nSee Figure 1 for a depiction of this composition.\\nConstraints. In the interest of having a fair comparison\\nacross child models, we limit our search to only architec-\\ntures con\\ufb01gurations that can contain between 59.1 million\\nand 64.1 million parameters when their relative output di-\\nmensions are scaled; in the Tensor2Tensor (Vaswani et al.,\\n2018) implementation we use, the base Transformer has\\nroughly 61.1 million parameters on WMT En-De, so our\\nmodels are allowed 3 million less or more parameters than\\nthat. Models that cannot be represented within this parame-\\nThe Evolved Transformer\\nter range are not included in our search space.\\nAdditionally, in preliminary experiment runs testing the\\neffectiveness of our search space, we discovered three trends\\nthat hurt performance in almost every case. Firstly and most\\nobviously is when a proposed decoder contains no ATTEND\\nTO ENCODER layers. This results in the decoder receiving\\nno signal from the encoder and thus the model output will\\nnot be conditioned on the input. Therefore, any model\\nthat does not contain ATTEND TO ENCODER is not in our\\nsearch space. The second trend that we noticed was that\\nmodels that had layer normalization removed were largely\\nworse than their parent models. For this reason, we remove\\nNONE from the normalization mutation vocabulary for each\\nexperiment, unless otherwise speci\\ufb01ed. Lastly, we observed\\nthat an important feature of good models was containing\\nan unbroken residual path from inputs to outputs; in our\\nsearch space, this means a path of IDENTITY layers from cell\\ninput to output that are combined with ADDITION at every\\ncombination function along the way. Our \\ufb01nal constraint\\nis therefore that models that do not have unbroken residual\\npaths from cell inputs to outputs are not in our search space.\\nC. Ablation Study of the Evolved\\nTransformer\\nTo understand what mutations contributed to ET\\u2019s improved\\nperformance we conducted two rounds of ablation testing.\\nIn the \\ufb01rst round, we began with the Transformer and ap-\\nplied each mutation to it individually to measure the per-\\nformance change each mutation introduces in isolation. In\\nthe second round, we began with ET and removed each\\nmutation individually to again measure the impact of each\\nsingle mutation. In both cases, each model was trained 3\\ntimes on WMT En-De for 300k steps with identical hyper-\\nparameters, using the inverse-square-root decay to 0 that\\nthe Transformer prefers. Each training was conducted on\\na single TPU V .2 chip. The results of these experiments\\nare presented in Table 5; we use validation perplexity for\\ncomparison because it was our \\ufb01tness metric.\\nIn all cases, the augmented ET models outperformed the\\nthe augmented Transformer models, indicating that the gap\\nin performance between ET and the Transformer cannot be\\nattributed to any single mutation. The mutation with the\\nseemingly strongest individual impact is the increase from\\n3 to 4 decoder cells. However, even when this mutation is\\nintroduced to the Transformer and removed from ET, the\\nresulting augmented ET model still has a higher \\ufb01tness than\\nthe augmented Transformer model.\\nTo highlight the impact of each augmented model\\u2019s mu-\\ntation, we present not only their perplexities but also the\\ndifference between their mean perplexity and their unaug-\\nmented base model mean perplexity in the \\u201dMean Diff\\u201dcolumns:\\nbase model mean perplexity -augmented mean perplexity\\nThis delta estimates the change in performance each mu-\\ntation creates in isolation. Red highlighted cells contain\\nevidence that their corresponding mutation hurt overall per-\\nformance. Green highlighted cells contain evidence that\\ntheir corresponding mutation helped overall performance.\\nIn half of the cases, both the augmented Transformer\\u2019s and\\nthe augmented Evolved Transformer\\u2019s performances indi-\\ncate that the mutation was helpful. Changing the number\\nof attention heads from 8 to 16 was doubly indicated to be\\nneutral and changing from 8 head self attention to a GLU\\nlayer in the decoder was doubly indicated to have hurt per-\\nformance. However, this and other mutations that seemingly\\nhurt performance may have been necessary given how we\\nformulate the problem: \\ufb01nding an improved model with a\\ncomparable number of parameters to the Transformer. For\\nexample, when the Transformer decoder cell is repeated 4\\ntimes, the resulting model has 69.6M parameters, which\\nis outside of our allowed parameter range. Thus, muta-\\ntions that shrank ET\\u2019s total number of parameters, even at a\\nslight degradation of performance, were necessary so that\\nother more impactful parameter-expensive mutations, such\\nas adding an additional decoder cell, could be used.\\nOther mutations have inconsistent evidence about how use-\\nful they are. This ablation study serves only to approximate\\nwhat is useful, but how effective a mutation is also depends\\non the model it is being introduced to and how it interacts\\nwith other encoding \\ufb01eld values.\\nThe Evolved Transformer\\nMUTATION FIELDMUTATION\\nBLOCK INDEXMUTATION\\nBRANCHTRANSFORMER VALUE ET V ALUETRANSFORMER\\nPERPLEXITYET P ERPLEXITYTRANSFORMER\\nMEAN DIFFET\\nMEAN DIFF\\nDECODER ACTIVATION 6 LEFT RELU S WISH 4.73\\u00060.01 4.51 \\u00060.02 -0.02 0.04\\nDECODER ACTIVATION 2 RIGHT RELU N ONE 4.73\\u00060.01 4.48 \\u00060.00 -0.02 0.02\\nDECODER INPUT 1 LEFT 1 0 4.74 \\u00060.04 4.46 \\u00060.00 -0.01 -0.01\\nDECODER LAYER 0 LEFT 8HEAD ATTENTION 16HEAD ATTENTION 4.75\\u00060.01 4.47 \\u00060.01 0.0 0.0\\nDECODER LAYER 2 LEFT STANDARD CONV 1X1 SEPARABLE CONV 11X1 4.67 \\u00060.01 4.55 \\u00060.00 -0.08 0.09\\nDECODER LAYER 3 LEFT STANDARD CONV 1X1 SEPARABLE CONV 7X1 4.72 \\u00060.01 4.46 \\u00060.01 -0.03 0.0\\nDECODER LAYER 2 RIGHT DEAD BRANCH SEPARABLE CONV 7X1 4.71 \\u00060.02 4.47 \\u00060.00 -0.04 0.01\\nDECODER NORM 3 LEFT NONE LAYER NORM 4.73\\u00060.00 4.45 \\u00060.01 -0.02 -0.01\\nDECODER NORM 7 LEFT NONE LAYER NORM 4.73\\u00060.02 4.47 \\u00060.02 -0.02 0.01\\nDECODER OUTPUT DIM 2 LEFT 8 4 4.74 \\u00060.01 4.45 \\u00060.01 -0.01 -0.02\\nDECODER NUM CELLS - - 3 4 4.62 \\u00060.00 4.59 \\u00060.01 -0.13 0.12\\nENCODER LAYERS 0 LEFT 8HEAD ATTENTION GATED LINEAR UNIT 4.80\\u00060.03 4.45 \\u00060.02 0.05 -0.01\\nENCODER LAYERS 2 LEFT STANDARD CONV 1X1 SEPARABLE CONV 9X1 4.69 \\u00060.01 4.50 \\u00060.00 -0.06 0.04\\nENCODER LAYERS 1 RIGHT DEAD BRANCH STANDARD CONV 3X1 4.73 \\u00060.01 4.47 \\u00060.03 -0.02 0.01\\nENCODER NORMS 2 LEFT NONE LAYER NORM 4.79\\u00060.03 4.46 \\u00060.02 0.04 0.0\\nENCODER OUTPUT DIM 2 LEFT 2 1 4.74 \\u00060.01 4.45 \\u00060.0 -0.01 -0.01\\nTable 5. Mutation Ablations : Each mutation is described by the \\ufb01rst 5 columns. The augmented Transformer and augmented ET\\nperplexities on the WMT\\u201914 En-De validation set are given in columns 6 and 7. Columns 7 and 8 show the difference between the\\nunaugmented base model perplexity mean and the augmented model perplexity mean. Red highlighted cells indicate evidence that the\\ncorresponding mutation hurts overall performance. Green highlighted cells indicate evidence that the corresponding mutation helps\\noverall performance.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"references\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":43},"id":"YGH7yKl_ARMD","outputId":"cdf7d6d9-71a4-47dd-dc00-c336cf37f8bd","executionInfo":{"status":"ok","timestamp":1715081582808,"user_tz":-180,"elapsed":24,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["(423, 13)"]},"metadata":{},"execution_count":6}],"source":["dataset.shape"]},{"cell_type":"code","source":["dataset.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":533},"id":"iorgiDdR4KwS","outputId":"cda62795-545d-477c-ab63-7be43dc2273c","executionInfo":{"status":"ok","timestamp":1715081582808,"user_tz":-180,"elapsed":21,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 423 entries, 0 to 422\n","Data columns (total 13 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   id                423 non-null    object\n"," 1   title             423 non-null    object\n"," 2   summary           423 non-null    object\n"," 3   source            423 non-null    object\n"," 4   authors           423 non-null    object\n"," 5   categories        423 non-null    object\n"," 6   comment           240 non-null    object\n"," 7   journal_ref       14 non-null     object\n"," 8   primary_category  423 non-null    object\n"," 9   published         423 non-null    object\n"," 10  updated           423 non-null    object\n"," 11  content           423 non-null    object\n"," 12  references        423 non-null    object\n","dtypes: object(13)\n","memory usage: 43.1+ KB\n"]}]},{"cell_type":"code","source":["# drop columns that we do not need\n","df_dataset=dataset.drop(columns = ['id', 'title',  'source', 'authors',  'comment',\n","       'journal_ref', 'primary_category', 'published', 'updated', 'content',\n","       'references'])\n","df_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":592},"id":"YuTBobwvIUKM","outputId":"9bd29aac-ab62-45ab-94ab-bcf718a28909","executionInfo":{"status":"ok","timestamp":1715081582809,"user_tz":-180,"elapsed":20,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["                                               summary  \\\n","0    Large language models (LLMs) have shown except...   \n","1    L$_2$ regularization and weight decay regulari...   \n","2    Stable Diffusion revolutionised image creation...   \n","3    Large language models (LLMs) have been shown t...   \n","4    Machine translation (MT) technology has facili...   \n","..                                                 ...   \n","418  We introduce GPT-NeoX-20B, a 20 billion parame...   \n","419  We propose a new family of policy gradient met...   \n","420  In this work, we develop and release Llama 2, ...   \n","421  Large language models (LLMs) have been shown t...   \n","422  This paper presents the first consumer-scale n...   \n","\n","                              categories  \n","0                         [cs.LG, cs.AI]  \n","1                [cs.LG, cs.NE, math.OC]  \n","2    [cs.LG, cs.AI, cs.CL, cs.CR, cs.CV]  \n","3                         [cs.AI, cs.CL]  \n","4                                [cs.CL]  \n","..                                   ...  \n","418                              [cs.CL]  \n","419                              [cs.LG]  \n","420                       [cs.CL, cs.AI]  \n","421                              [cs.CL]  \n","422              [cs.LG, cs.CR, stat.ML]  \n","\n","[423 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-9f9c26fe-c1b6-4260-a593-c97aed666f6e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>summary</th>\n","      <th>categories</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Large language models (LLMs) have shown except...</td>\n","      <td>[cs.LG, cs.AI]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>L$_2$ regularization and weight decay regulari...</td>\n","      <td>[cs.LG, cs.NE, math.OC]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Stable Diffusion revolutionised image creation...</td>\n","      <td>[cs.LG, cs.AI, cs.CL, cs.CR, cs.CV]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Large language models (LLMs) have been shown t...</td>\n","      <td>[cs.AI, cs.CL]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Machine translation (MT) technology has facili...</td>\n","      <td>[cs.CL]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>418</th>\n","      <td>We introduce GPT-NeoX-20B, a 20 billion parame...</td>\n","      <td>[cs.CL]</td>\n","    </tr>\n","    <tr>\n","      <th>419</th>\n","      <td>We propose a new family of policy gradient met...</td>\n","      <td>[cs.LG]</td>\n","    </tr>\n","    <tr>\n","      <th>420</th>\n","      <td>In this work, we develop and release Llama 2, ...</td>\n","      <td>[cs.CL, cs.AI]</td>\n","    </tr>\n","    <tr>\n","      <th>421</th>\n","      <td>Large language models (LLMs) have been shown t...</td>\n","      <td>[cs.CL]</td>\n","    </tr>\n","    <tr>\n","      <th>422</th>\n","      <td>This paper presents the first consumer-scale n...</td>\n","      <td>[cs.LG, cs.CR, stat.ML]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>423 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f9c26fe-c1b6-4260-a593-c97aed666f6e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9f9c26fe-c1b6-4260-a593-c97aed666f6e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9f9c26fe-c1b6-4260-a593-c97aed666f6e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-9e93a075-0646-43f4-a605-00cad36b71f4\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9e93a075-0646-43f4-a605-00cad36b71f4')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-9e93a075-0646-43f4-a605-00cad36b71f4 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_b5dde77d-b357-4048-a335-e50ad42dfb4d\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_dataset')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_b5dde77d-b357-4048-a335-e50ad42dfb4d button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_dataset');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_dataset","summary":"{\n  \"name\": \"df_dataset\",\n  \"rows\": 423,\n  \"fields\": [\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 423,\n        \"samples\": [\n          \"The original ImageNet dataset is a popular large-scale benchmark for training\\nDeep Neural Networks. Since the cost of performing experiments (e.g, algorithm\\ndesign, architecture search, and hyperparameter tuning) on the original dataset\\nmight be prohibitive, we propose to consider a downsampled version of ImageNet.\\nIn contrast to the CIFAR datasets and earlier downsampled versions of ImageNet,\\nour proposed ImageNet32$\\\\times$32 (and its variants ImageNet64$\\\\times$64 and\\nImageNet16$\\\\times$16) contains exactly the same number of classes and images as\\nImageNet, with the only difference that the images are downsampled to\\n32$\\\\times$32 pixels per image (64$\\\\times$64 and 16$\\\\times$16 pixels for the\\nvariants, respectively). Experiments on these downsampled variants are\\ndramatically faster than on the original ImageNet and the characteristics of\\nthe downsampled datasets with respect to optimal hyperparameters appear to\\nremain similar. The proposed datasets and scripts to reproduce our results are\\navailable at http://image-net.org/download-images and\\nhttps://github.com/PatrykChrabaszcz/Imagenet32_Scripts\",\n          \"Transformer based language models (LMs) demonstrate increasing performance\\nwith scale across a wide variety of tasks. Scale alone however cannot enable\\nmodels to solve tasks that require access to ephemeral, changing, or private\\ndata that was unavailable at training time. Many useful tasks may also benefit\\nfrom LMs being able to access APIs that read or modify state. In this work, we\\npresent Tool Augmented Language Models (TALM), combining a text-only approach\\nto augment language models with non-differentiable tools, and an iterative\\n\\\"self-play\\\" technique to bootstrap performance starting from few tool\\ndemonstrations. TALM exhibits strong performance on both a knowledge-heavy QA\\ntask and a reasoning oriented math task with simple tools. At a given model\\nscale, TALM significantly outperforms non-augmented LMs. We further demonstrate\\nthat TALM successfully performs out-of-distribution inferences on both QA and\\nmath tasks, where non-augmented LMs fail. Our results suggest that Tool\\nAugmented Language Models are a promising direction to enrich LMs'\\ncapabilities, with less dependence on scale.\",\n          \"Recent works have highlighted the strength of the Transformer architecture on\\nsequence tasks while, at the same time, neural architecture search (NAS) has\\nbegun to outperform human-designed models. Our goal is to apply NAS to search\\nfor a better alternative to the Transformer. We first construct a large search\\nspace inspired by the recent advances in feed-forward sequence models and then\\nrun evolutionary architecture search with warm starting by seeding our initial\\npopulation with the Transformer. To directly search on the computationally\\nexpensive WMT 2014 English-German translation task, we develop the Progressive\\nDynamic Hurdles method, which allows us to dynamically allocate more resources\\nto more promising candidate models. The architecture found in our experiments\\n-- the Evolved Transformer -- demonstrates consistent improvement over the\\nTransformer on four well-established language tasks: WMT 2014 English-German,\\nWMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size,\\nthe Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8\\non WMT'14 English-German; at smaller sizes, it achieves the same quality as the\\noriginal \\\"big\\\" Transformer with 37.6% less parameters and outperforms the\\nTransformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Summary Pre-processing\n","\n","def preprocess_text(text):\n","    # convert text to lower case\n","    text = text.lower()\n","    # split text into tokens to remove whitespaces\n","    words = word_tokenize(text)\n","    # Remove special characters and punctuation using regular expression\n","    words = [re.sub(r'[^a-zA-Z]', '', word) for word in words]  # Removes numbers and special characters\n","\n","    return \" \".join(words)\n","\n","df_dataset['summary'] = df_dataset['summary'].apply(preprocess_text)\n","df_dataset['summary'][1]"],"metadata":{"id":"FRxRJYbCIcAp","colab":{"base_uri":"https://localhost:8080/","height":198},"outputId":"9eab1478-b922-4113-f644-1f81d4b4f4e7","executionInfo":{"status":"ok","timestamp":1715081585234,"user_tz":-180,"elapsed":2442,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["'l    regularization and weight decay regularization are equivalent for standard stochastic gradient descent  when rescaled by the learning rate   but as we demonstrate this is emph  not  the case for adaptive gradient algorithms  such as adam  while common implementations of these algorithms employ l    regularization  often calling it  weight decay  in what may be misleading due to the inequivalence we expose   we propose a simple modification to recover the original formulation of weight decay regularization by emph  decoupling  the weight decay from the optimization steps taken wrt  the loss function  we provide empirical evidence that our proposed modification  i  decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard sgd and adam and  ii  substantially improves adams generalization performance  allowing it to compete with sgd with momentum on image classification datasets  on which it was previously typically outperformed by the latter   our proposed decoupled weight decay has already been adopted by many researchers  and the community has implemented it in tensorflow and pytorch  the complete source code for our experiments is available at https  githubcomloshchiladamwandsgdw'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Check data types of values in the 'categories' column (one of the Data Challenges that have been solved and this is a check to see if the problem still existe or not (problem of NaN type of 'categories' column))\n","\n","print(df_dataset['categories'].dtype)\n","types = df_dataset['categories'].apply(type)\n","types.value_counts()#to check if all the categoris of the same type and if it has NaN values or not for later processing(converting categories into a a binary representation)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120},"id":"2ZCaRxW1NrHJ","outputId":"ea898bc9-6b75-4e7e-bad3-bbbb412fdb61","executionInfo":{"status":"ok","timestamp":1715081585235,"user_tz":-180,"elapsed":22,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["object\n"]},{"output_type":"execute_result","data":{"text/plain":["categories\n","<class 'list'>    423\n","Name: count, dtype: int64"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["df_dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":592},"id":"nqNjlL1wJ7GM","outputId":"62f6dd4d-21cf-4e70-a025-9b945af70491","executionInfo":{"status":"ok","timestamp":1715081585235,"user_tz":-180,"elapsed":19,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["                                               summary  \\\n","0    large language models  llms  have shown except...   \n","1    l    regularization and weight decay regulariz...   \n","2    stable diffusion revolutionised image creation...   \n","3    large language models  llms  have been shown t...   \n","4    machine translation  mt  technology has facili...   \n","..                                                 ...   \n","418  we introduce gptneoxb  a  billion parameter au...   \n","419  we propose a new family of policy gradient met...   \n","420  in this work  we develop and release llama   a...   \n","421  large language models  llms  have been shown t...   \n","422  this paper presents the first consumerscale ne...   \n","\n","                              categories  \n","0                         [cs.LG, cs.AI]  \n","1                [cs.LG, cs.NE, math.OC]  \n","2    [cs.LG, cs.AI, cs.CL, cs.CR, cs.CV]  \n","3                         [cs.AI, cs.CL]  \n","4                                [cs.CL]  \n","..                                   ...  \n","418                              [cs.CL]  \n","419                              [cs.LG]  \n","420                       [cs.CL, cs.AI]  \n","421                              [cs.CL]  \n","422              [cs.LG, cs.CR, stat.ML]  \n","\n","[423 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-9e25e670-d0d4-4154-a261-3ec0e4c069be\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>summary</th>\n","      <th>categories</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>large language models  llms  have shown except...</td>\n","      <td>[cs.LG, cs.AI]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>l    regularization and weight decay regulariz...</td>\n","      <td>[cs.LG, cs.NE, math.OC]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>stable diffusion revolutionised image creation...</td>\n","      <td>[cs.LG, cs.AI, cs.CL, cs.CR, cs.CV]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>large language models  llms  have been shown t...</td>\n","      <td>[cs.AI, cs.CL]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>machine translation  mt  technology has facili...</td>\n","      <td>[cs.CL]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>418</th>\n","      <td>we introduce gptneoxb  a  billion parameter au...</td>\n","      <td>[cs.CL]</td>\n","    </tr>\n","    <tr>\n","      <th>419</th>\n","      <td>we propose a new family of policy gradient met...</td>\n","      <td>[cs.LG]</td>\n","    </tr>\n","    <tr>\n","      <th>420</th>\n","      <td>in this work  we develop and release llama   a...</td>\n","      <td>[cs.CL, cs.AI]</td>\n","    </tr>\n","    <tr>\n","      <th>421</th>\n","      <td>large language models  llms  have been shown t...</td>\n","      <td>[cs.CL]</td>\n","    </tr>\n","    <tr>\n","      <th>422</th>\n","      <td>this paper presents the first consumerscale ne...</td>\n","      <td>[cs.LG, cs.CR, stat.ML]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>423 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e25e670-d0d4-4154-a261-3ec0e4c069be')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9e25e670-d0d4-4154-a261-3ec0e4c069be button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9e25e670-d0d4-4154-a261-3ec0e4c069be');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-671b4816-0720-48fe-8003-fea359e57e5d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-671b4816-0720-48fe-8003-fea359e57e5d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-671b4816-0720-48fe-8003-fea359e57e5d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_8d05cdaf-8389-4aa7-81af-22be4fd279e9\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_dataset')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_8d05cdaf-8389-4aa7-81af-22be4fd279e9 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_dataset');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_dataset","summary":"{\n  \"name\": \"df_dataset\",\n  \"rows\": 423,\n  \"fields\": [\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 423,\n        \"samples\": [\n          \"the original imagenet dataset is a popular largescale benchmark for training deep neural networks  since the cost of performing experiments  eg  algorithm design  architecture search  and hyperparameter tuning  on the original dataset might be prohibitive  we propose to consider a downsampled version of imagenet  in contrast to the cifar datasets and earlier downsampled versions of imagenet  our proposed imagenet  times    and its variants imagenet  times   and imagenet  times    contains exactly the same number of classes and images as imagenet  with the only difference that the images are downsampled to   times   pixels per image    times   and   times   pixels for the variants  respectively   experiments on these downsampled variants are dramatically faster than on the original imagenet and the characteristics of the downsampled datasets with respect to optimal hyperparameters appear to remain similar  the proposed datasets and scripts to reproduce our results are available at http  imagenetorgdownloadimages and https  githubcompatrykchrabaszczimagenetscripts\",\n          \"transformer based language models  lms  demonstrate increasing performance with scale across a wide variety of tasks  scale alone however can not enable models to solve tasks that require access to ephemeral  changing  or private data that was unavailable at training time  many useful tasks may also benefit from lms being able to access apis that read or modify state  in this work  we present tool augmented language models  talm   combining a textonly approach to augment language models with nondifferentiable tools  and an iterative  selfplay  technique to bootstrap performance starting from few tool demonstrations  talm exhibits strong performance on both a knowledgeheavy qa task and a reasoning oriented math task with simple tools  at a given model scale  talm significantly outperforms nonaugmented lms  we further demonstrate that talm successfully performs outofdistribution inferences on both qa and math tasks  where nonaugmented lms fail  our results suggest that tool augmented language models are a promising direction to enrich lms capabilities  with less dependence on scale \",\n          \"recent works have highlighted the strength of the transformer architecture on sequence tasks while  at the same time  neural architecture search  nas  has begun to outperform humandesigned models  our goal is to apply nas to search for a better alternative to the transformer  we first construct a large search space inspired by the recent advances in feedforward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the transformer  to directly search on the computationally expensive wmt  englishgerman translation task  we develop the progressive dynamic hurdles method  which allows us to dynamically allocate more resources to more promising candidate models  the architecture found in our experiments  the evolved transformer  demonstrates consistent improvement over the transformer on four wellestablished language tasks  wmt  englishgerman  wmt  englishfrench  wmt  englishczech and lmb  at a big model size  the evolved transformer establishes a new stateoftheart bleu score of  on wmt englishgerman  at smaller sizes  it achieves the same quality as the original  big  transformer with   less parameters and outperforms the transformer by  bleu at a mobilefriendly model size of m parameters \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# compute no. of words in each summary to investegate the need of token turncation\n","# DistilBERT max leangth is 512 token so max length of summaries in dataset should be <= DistilBert max token leangth\n","\n","summary = df_dataset['summary']\n","word_cnt = [len(summ.split()) for summ in summary]\n","# Plot the distribution\n","plt.figure(figsize=[8,5])\n","plt.hist(word_cnt, bins = 40)\n","plt.xlabel('Word Count/summary')\n","plt.ylabel('# of Occurences')\n","plt.title(\"Frequency of Word Counts/sentence\")\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"id":"8wCEIWqfKHfA","outputId":"f82e8663-4ebf-42cb-c381-5c81f65a9c59","executionInfo":{"status":"ok","timestamp":1715081586303,"user_tz":-180,"elapsed":1084,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2AABKhv1AQ8H","outputId":"c0e1302b-be10-4ae5-e02a-d1fae6a8d3e5","executionInfo":{"status":"ok","timestamp":1715081586303,"user_tz":-180,"elapsed":24,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["cs.CL                            314\n","cs.LG                            203\n","cs.AI                            161\n","stat.ML                           54\n","cs.CY                             35\n","cs.CV                             32\n","cs.NE                             10\n","cs.CR                             10\n","cs.RO                              8\n","cs.IR                              5\n","cs.SE                              5\n","cs.HC                              4\n","cs.SI                              4\n","I.2.7                              4\n","cs.PL                              3\n","eess.AS                            2\n","cs.SD                              2\n","cs.MA                              2\n","q-bio.NC                           2\n","I.2.6; I.2.7                       2\n","math.OC                            2\n","cs.AR                              2\n","68T50, 68T09, 91D30                1\n","cs.PF                              1\n","q-fin.EC                           1\n","econ.GN                            1\n","I.2; I.2.7                         1\n","68-06, 68-04, 68T50, 68T01         1\n","68T50                              1\n","68T50, 68T07                       1\n","I.2; I.7; K.4                      1\n","physics.geo-ph                     1\n","I.2; I.7                           1\n","cs.CL, cs.GL                       1\n","68-04, 68-06, 68T50                1\n","G.3; I.2.7                         1\n","cs.DC                              1\n","cs.SY                              1\n","cond-mat.dis-nn                    1\n","I.2                                1\n","eess.IV                            1\n","cs.GT                              1\n","cs.DS                              1\n","q-bio.QM                           1\n","I.2.6; I.2.8; I.5.0; J.2; J.3      1\n","cs.CG                              1\n","I.5.1; I.2.7                       1\n","cs.SC                              1\n","Name: count, dtype: int64"]},"metadata":{},"execution_count":13}],"source":["# category investegation\n","categories_counts = [c for cat in df_dataset['categories'] for c in cat]\n","# count frequency of each single category\n","pd.Series(categories_counts).value_counts()\n","#there is a bias in data in which the model will tend to pridict the most frequent categories more often than the rest\n","#data contains impalenced class labels which might affect preformance(for later investgtion)"]},{"cell_type":"markdown","metadata":{"id":"UMCDaPyGHJa3"},"source":["## Label Encoder"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"WbFj3A21AQxP","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1715081586304,"user_tz":-180,"elapsed":21,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}},"outputId":"bab0f058-ccc7-4837-fa36-97c46cea5959"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}}],"source":["# Encode the categories(labels) in a float format in order to be used for training & inference\n","multilabel = MultiLabelBinarizer()\n","\n","labels = multilabel.fit_transform(df_dataset['categories']).astype('float32')\n","\n","texts = df_dataset['summary'].tolist()"]},{"cell_type":"code","source":["labels.shape # 48 is the number of unique labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":43},"id":"rHeq7TKiaogp","outputId":"87ebabf2-8f3e-467c-85b9-118b942e56b4","executionInfo":{"status":"ok","timestamp":1715081586304,"user_tz":-180,"elapsed":19,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["(423, 48)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# To check the result of encoding\n","print(\"categories of first summary: \",df_dataset['categories'][0])\n","print(\"one-hot encoding of categories of first summary: \",labels[0])\n","print(multilabel.inverse_transform(labels[0].reshape(1,-1)))\n","print(multilabel.classes_[28])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":146},"id":"o1tuj3usbffO","outputId":"c0148da5-3079-4e42-da9e-dc381bb9113a","executionInfo":{"status":"ok","timestamp":1715081586916,"user_tz":-180,"elapsed":628,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["categories of first summary:  ['cs.LG', 'cs.AI']\n","one-hot encoding of categories of first summary:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","[('cs.AI', 'cs.LG')]\n","cs.LG\n"]}]},{"cell_type":"code","source":["texts[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"aYuBMJVYRxne","outputId":"86f295b7-fa65-4e5b-d6c1-54cd996639e0","executionInfo":{"status":"ok","timestamp":1715081586917,"user_tz":-180,"elapsed":17,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}}},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["['large language models  llms  have shown exceptional performance on a variety of natural language tasks  yet  their capabilities for html understanding  ie  parsing the raw html of a webpage  with applications to automation of webbased tasks  crawling  and browserassisted retrieval  have not been fully explored  we contribute html understanding models  finetuned llms  and an indepth analysis of their capabilities under three tasks   i  semantic classification of html elements   ii  description generation for html inputs  and  iii  autonomous web navigation of html pages  while previous work has developed dedicated architectures and training procedures for html understanding  we show that llms pretrained on standard natural language corpora transfer remarkably well to html understanding tasks  for instance  finetuned llms are   more accurate at semantic classification compared to models trained exclusively on the task dataset  moreover  when finetuned on data from the miniwob benchmark  llms successfully complete   more tasks using x less data compared to the previous best supervised model  out of the llms we evaluate  we show evidence that tbased models are ideal due to their bidirectional encoderdecoder architecture  to promote further research on llms for html understanding  we create and opensource a largescale html dataset distilled and autolabeled from commoncrawl ',\n"," 'l    regularization and weight decay regularization are equivalent for standard stochastic gradient descent  when rescaled by the learning rate   but as we demonstrate this is emph  not  the case for adaptive gradient algorithms  such as adam  while common implementations of these algorithms employ l    regularization  often calling it  weight decay  in what may be misleading due to the inequivalence we expose   we propose a simple modification to recover the original formulation of weight decay regularization by emph  decoupling  the weight decay from the optimization steps taken wrt  the loss function  we provide empirical evidence that our proposed modification  i  decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard sgd and adam and  ii  substantially improves adams generalization performance  allowing it to compete with sgd with momentum on image classification datasets  on which it was previously typically outperformed by the latter   our proposed decoupled weight decay has already been adopted by many researchers  and the community has implemented it in tensorflow and pytorch  the complete source code for our experiments is available at https  githubcomloshchiladamwandsgdw',\n"," 'stable diffusion revolutionised image creation from descriptive text  gpt  gpt    and gpt demonstrated astonishing performance across a variety of language tasks  chatgpt introduced such language models to the general public  it is now clear that large language models  llms  are here to stay  and will bring about drastic change in the whole ecosystem of online text and images  in this paper we consider what the future might hold  what will happen to gpt  n  once llms contribute much of the language found online  we find that use of modelgenerated content in training causes irreversible defects in the resulting models  where tails of the original content distribution disappear  we refer to this effect as model collapse and show that it can occur in variational autoencoders  gaussian mixture models and llms  we build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models  we demonstrate that it has to be taken seriously if we are to sustain the benefits of training from largescale data scraped from the web  indeed  the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by llms in data crawled from the internet ',\n"," 'large language models  llms  have been shown to be capable of impressive fewshot generalisation to new tasks  however  they still tend to perform poorly on multistep logical reasoning problems  here we carry out a comprehensive evaluation of llms on  tasks that probe different aspects of logical reasoning  we show that language models tend to perform fairly well at single step inference or entailment tasks  but struggle to chain together multiple reasoning steps to solve more complex problems  in light of this  we propose a selectioninference  si  framework that exploits pretrained llms as general processing modules  and alternates between selection and inference to generate a series of interpretable  casual reasoning steps leading to the final answer  we show that a b parameter llm used within the si framework in a shot generalisation setting  with no finetuning  yields a performance improvement of over   compared to an equivalent vanilla baseline on a suite of  logical reasoning tasks  the same model in the same setting even outperforms a significantly larger b parameter baseline on the same suite of tasks  moreover  answers produced by the si framework are accompanied by a causal naturallanguagebased reasoning trace  which has important implications for the safety and trustworthiness of the system ',\n"," 'machine translation  mt  technology has facilitated our daily tasks by providing accessible shortcuts for gathering  elaborating and communicating information  however  it can suffer from biases that harm users and society at large  as a relatively new field of inquiry  gender bias in mt still lacks internal cohesion  which advocates for a unified framework to ease future research  to this end  we  i  critically review current conceptualizations of bias in light of theoretical insights from related disciplines  ii  summarize previous analyses aimed at assessing gender bias in mt  iii  discuss the mitigating strategies proposed so far  and iv  point toward potential directions for future work ']"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["## Data Split"],"metadata":{"id":"x3sWBcp-uG3g"}},{"cell_type":"code","source":["# Split data into training and testing sets with a ratio of (90% train, 10% test) because we have very small data so we need higher train split\n","X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.10, random_state=42)\n","\n","# Further split the training set into training and validation sets (split ratio 90%, 10% validation) so we can have a data split ratio of (80, 10, 10)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)"],"metadata":{"id":"gS_PkS06TzLY","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1715081586917,"user_tz":-180,"elapsed":15,"user":{"displayName":"Wajd Alrabiah","userId":"10048035396885890619"}},"outputId":"cd8f33e5-a27f-4c32-846e-a5987fa4d6c1"},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style> \n","    body {\n","      font-size: 20px;\n","    }\n","  "]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"XYz_Y6ARIU8j"},"source":["# **Model Building**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232,"referenced_widgets":["9ac6524909a64d7cb22552578aa6fd94","951d459f23834a14a05e2cd321399edc","e6b509d39a5445658b5c3af3b4156e2b","d20f232af3014e6c82f7a970fdc8caa4","31dfff3148da4fa0a0cd6b31db510200","9777b7bca93c4b06a8e8ed0bd109000d","e61abb2e7acf49ed807f2ca1d21c3de9","dfc491b710f64acc83a7e9dbdbaa4ac9","abc52c469cec40f89d0134f92ed12e04","49e1cfd4d7984e6e9aa4b7975d4c2e03","8959078bd6f3449b804919dba82ff8e5","ec158d9ca9d748d09c76e68d0abe6179","41c9432ef4cf47d3ae04e08231c68c84","2b8395ea76ef4d22bdbe6ed48671ebe8","34cb45732762488b8ca90595c6af3615","3ff6693d7bb24b94af62d35bb7af84a2","ab2cc703b49740f191d518004cf89003","480cb67125924e80b2738702a427a41a","070db4772de247cf8a725e8039b1df53","272f329ccdc7423cb450f2adb1f21a46","8736f9ecbe374d8097a09c9c5bf9526e","d1a2549ed0a94a1bab6bc158cf12f7e0","d175e3587b3b419183477e7014d11ae8","e1c76d47588144d08e06532234e8b025","3532e35f4a8f44dcbf4ee1e9602c494e","58555aea84bb4f5fa2008fc3012055c8","f8ce01e2eb2c4b368883f977972a50c4","6c39f8000e5342d580d738c7a866e432","8ea2baa3f9d348c6b1795c629336a759","15f0fcba5d874545bcecf1d2976b56d8","ab589495861247dfa640159f77bc57e8","b70996ea360c45c98dede9e47bfc7242","048cbb15c92f4cbab36b9503e870f713","62e4c7b817e245b594a3abe1052ff473","fbf1d870a44c4a9d904eaa638bb43723","d1f03bbd25e3441b830a608ba7f8636a","8eccd5e95f1942bdba8b095dde903193","3acee30794e54226ab9535e0b6347fd1","0e96b6a1fb7f4159968550bbc65b31db","67bd25eb217c4643b7a4495569e9f94d","5408283bb416486081b678b43f9cffa2","9d56be2960ee46c58f603cedc4bb5891","9f153d844e51402f9e88c62b6e7013e7","13b68a3615e843c8965cdf550e3c0d17","e1181a7edd984d2da5775e5c74011b6f","f07463f6388b469aa2efcc8ee6016ec9","a90980e7cf014f28aad749fcdbbc8f51","30281fb4ecee4c7189c8be2001dd8ddc","21fd7e2838314f589b9589afa0d45cd8","19a558f2a86e407aacd968fe20cda968","721322ae1eef486b80a48c6e82550bca","6521af96419148dcae08ba95d3738973","26246fa0035b4dc59e20e27f4eb976c4","be5bd05ffaac4de0b13966131da6a1dd","b0afe099c0294e818abfc16ba7a38d58"]},"id":"BYX8Ls1qAQj3","outputId":"b1cca81c-5d29-4f44-96ec-3b70116161b5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ac6524909a64d7cb22552578aa6fd94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec158d9ca9d748d09c76e68d0abe6179"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d175e3587b3b419183477e7014d11ae8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62e4c7b817e245b594a3abe1052ff473"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1181a7edd984d2da5775e5c74011b6f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["#load the model and tokenizer\n","checkpoint = \"distilbert-base-uncased\"\n","tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n","model = DistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=len(labels[0]), problem_type=\"multi_label_classification\")\n","\n","# This model has a classification head with the number of output neurons equal to the number of labels(e.g.len(labels[0])).\n","# These neurons have a softmax activation function, and they are responsible for predicting the probability of each label."]},{"cell_type":"markdown","source":["## Model Input Preparation"],"metadata":{"id":"g0EAG9tC3Zfl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cq8lJXg9AQcZ"},"outputs":[],"source":["# to process our data and transform custom dataset into a torch tensor to be fed into the model\n","class CustomDataset(Dataset): # Dataset is a torch class -> super abstract class that gets inhereted by CustomDataset class\n","\n","    def __init__(self, texts, labels, tokenizer, max_len=300):\n","      #max_len=128 as standard, the model is trained on a max sequece length of 512 or less due to computational constraints,\n","      #value of max_len could impact training efficiency; shorter sequences can lead to faster training and require fewer computations compared to longer sequences\n","      #data distribution:\n","      self.texts = texts\n","      self.labels = labels\n","      self.tokenizer = tokenizer\n","      self.max_len = max_len\n","\n","    def __len__(self):\n","      return len(self.texts)\n","\n","      #tokenization will happen here , this method will allow us to access individual samples when iterating over it using 'idx'\n","    def __getitem__(self, idx): #idx = index while iterating the dataset\n","       text = str(self.texts[idx]) #convert the text to a string to ensure compatibility with the tokenizer\n","       label = torch.tensor(self.labels[idx]) #convert the label to a pytorch tensor\n","       encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length = self.max_len, return_tensors='pt')\n","# truncation=True -> if text exceeds max_len=300, then truncate it.\n","# padding=\"max_length\" -> pads the exceeded sequence to the length of max_length =300\n","# max_length = self.max_len -> specifies the maximum length allowed of the tokenized text\n","# return_tensors='pt' -> returns the tokenized text as pytorch tensor.\n","# encoding -> dictionary containing the tokenized input, including 'input_ids' and 'attention_mask'\n","\n","       return {\n","           'input_ids': encoding['input_ids'].flatten(),\n","           'attention_mask': encoding['attention_mask'].flatten(),\n","           'labels': label\n","       }\n","# the last segment returns the dictionary along with the correspoinding label, The flatten() method is used to convert the\n","#tensors into one-dimensional tensors; to ensure compatibility with the neural network, as they expect input to  be a 1-D tensor. and to reduces memory overhead"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRnAqW6QAQZ0"},"outputs":[],"source":["train_dataset = CustomDataset(X_train, y_train, tokenizer)\n","validat_dataset = CustomDataset(X_val, y_val, tokenizer)\n","test_dataset = CustomDataset(X_test, y_test, tokenizer)"]},{"cell_type":"code","source":["train_dataset[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n3XFE_SMZW3j","outputId":"79b25a92-7914-445c-f929-f6f20fa78d6f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([  101,  2023,  5002,  4391,  2573,  1999,  2029,  2653,  4275,  1048,\n","          5244,  2024, 19335,  2007, 13384,  4813,  1998,  1996,  3754,  2000,\n","          2224,  5906,  1996,  2280,  2003,  4225,  2004, 21933,  8737, 18606,\n","          1037,  9280,  3375,  4708,  2046, 16325,  4942, 10230,  5705,  2096,\n","          1996,  3732,  3774,  1999,  4214,  6327, 14184,  2107,  2004,  1037,\n","          3642, 19555,  1048,  5244,  2064, 21155,  2122, 15476,  3672, 10708,\n","         10329,  2030,  1999,  5257,  3081,  2002,  9496, 10074,  2015,  2030,\n","          4553,  2000,  2079,  2061,  2013, 13616,  2096,  4748, 22658,  2000,\n","          1037,  3115,  4394, 19204,  2015, 17547,  7863,  2107, 19335,  1048,\n","          5244,  2064,  2224,  2536,  4298,  2512, 28689, 12589,  6327, 14184,\n","          2000,  7818,  2037,  6123,  6364,  3754,  2947, 15971,  2013,  1996,\n","          5760,  2653, 11643, 20680,  2057,  3568,  6523,  2000,  2068,  2004,\n","         19335,  2653,  4275,  2632,  5244,  1996,  4394, 19204,  7863,  4473,\n","          2632,  5244,  2000,  4553,  2000,  3114,  2224,  5906,  1998,  2130,\n","          2552,  2096,  2145,  4488,  3115,  3019,  2653,  8518,  1998,  2130,\n","          2041,  4842, 14192,  2075,  2087,  3180,  1048,  5244,  2006,  2195,\n","          6847, 27373,  1999,  2023,  2147,  2044, 15252,  2783,  5083,  1999,\n","          2632,  5244,  2057, 16519,  2008,  2023,  2047,  2470,  3257,  2038,\n","          1996,  4022,  2000,  4769,  2691, 12546,  1997,  3151,  1048,  5244,\n","          2107,  2004, 17841,  8010, 18700,  1998, 26743,  8553,  3314,   102,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n"," 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n"," 'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apwyGnyaAQVW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"427733ee-e092-4cc4-f37f-473e34447cee"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([  101,  2312,  2653,  4275,  2031,  2042,  3491,  2000,  6162,  9487,\n","          2836,  2408,  1037,  3528,  1997,  3019,  2653,  8518,  2478,  2261,\n","         19040,  4083,  2029, 21040, 13416,  1996,  2193,  1997,  8518,  5051,\n","          6895,  8873,  2278,  2731,  4973,  2734,  2000, 15581,  1996,  2944,\n","          2000,  1037,  3327,  4646,  2000,  2582,  2256,  4824,  1997,  1996,\n","          4254,  1997,  4094,  2006,  2261, 19040,  4083,  2057,  4738,  1037,\n","          4551, 16381, 19441,  8878, 10938,  2121,  2653,  2944,  2029,  2057,\n","          2655, 16910,  2653,  2944,  5340,  2057,  4738,  5340,  2006,  1056,\n","         14289,  1058, 11772,  2478, 16910,  1037,  2047, 19875,  2291,  2029,\n","         12939,  3811,  8114,  2731,  2408,  3674,  1056, 14289, 26723,  2057,\n","         10580,  2506,  6666,  1997, 25169,  2011, 10910,  2110, 15794, 22375,\n","          2261, 19040,  4083,  3463,  2006,  5606,  1997,  2653,  4824,  1998,\n","          4245,  6847, 27373,  2006,  1037,  2193,  1997,  2122,  8518,  5340,\n","          1038,  6162,  2015, 12687,  2836,  2041,  4842, 14192,  2075,  1996,\n","          2986,  8525,  7228,  2110, 15794, 22375,  2006,  1037,  7621,  1997,\n","          4800, 13473,  2361, 13384,  8518,  1998,  2041,  4842, 14192,  2075,\n","          2779,  2529,  2836,  2006,  1996,  3728,  2207,  2502, 10609,  2818,\n","          6847, 10665,  1037,  3278,  2193,  1997,  2502, 10609,  2818,  8518,\n","          3662, 12532, 16778, 11231,  3560,  8377,  2013,  2944,  4094,  3574,\n","          2008,  2836,  9561,  2135,  3445,  2004,  2057, 18953,  2000,  2256,\n","          2922,  2944,  5340,  2036,  2038,  2844,  9859,  1999,  4800,  2989,\n","          8787,  8518,  1998,  3120,  3642,  4245,  2029,  2057, 10580,  2006,\n","          1037,  2898,  9140,  1997,  6847, 27373,  2057,  5678,  3073,  1037,\n","          7721,  4106,  2006, 13827,  1998, 22423,  1998,  2817,  1996,  6698,\n","          1997,  2731,  2951, 24443, 26910,  2007,  4847,  2000,  2944,  4094,\n","          2633,  2057,  6848,  1996, 12962, 16852,  3141,  2000,  2312,  2653,\n","          4275,  1998,  6848,  4022, 10210, 13340,  3508,  9942,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n"," 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n"," 'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"]},"metadata":{},"execution_count":22}],"source":["validat_dataset[0]"]},{"cell_type":"code","source":["test_dataset[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vuAFK5M1r0_r","outputId":"29b84cf6-7751-4929-e886-d058376f1959"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': tensor([  101,  1996,  2434,  3746,  7159,  2951, 13462,  2003,  1037,  2759,\n","          2312, 15782,  2571,  6847, 10665,  2005,  2731,  2784, 15756,  6125,\n","          2144,  1996,  3465,  1997,  4488,  7885,  1041,  2290,  9896,  2640,\n","          4294,  3945,  1998, 23760, 28689, 22828, 17372,  2006,  1996,  2434,\n","          2951, 13462,  2453,  2022, 23469,  3512,  2057, 16599,  2000,  5136,\n","          1037, 12482, 16613,  3709,  2544,  1997,  3746,  7159,  1999,  5688,\n","          2000,  1996, 25022, 14971,  2951, 13462,  2015,  1998,  3041, 12482,\n","         16613,  3709,  4617,  1997,  3746,  7159,  2256,  3818,  3746,  7159,\n","          2335,  1998,  2049, 10176,  3746,  7159,  2335,  1998,  3746,  7159,\n","          2335,  3397,  3599,  1996,  2168,  2193,  1997,  4280,  1998,  4871,\n","          2004,  3746,  7159,  2007,  1996,  2069,  4489,  2008,  1996,  4871,\n","          2024, 12482, 16613,  3709,  2000,  2335, 27725,  2566,  3746,  2335,\n","          1998,  2335, 27725,  2005,  1996, 10176,  4414,  7885,  2006,  2122,\n","         12482, 16613,  3709, 10176,  2024, 12099,  5514,  2084,  2006,  1996,\n","          2434,  3746,  7159,  1998,  1996,  6459,  1997,  1996, 12482, 16613,\n","          3709,  2951, 13462,  2015,  2007,  4847,  2000, 15502, 23760, 28689,\n","         22828,  2015,  3711,  2000,  3961,  2714,  1996,  3818,  2951, 13462,\n","          2015,  1998, 14546,  2000, 21376,  2256,  3463,  2024,  2800,  2012,\n","          8299,  3746,  7159, 21759,  7698, 11066,  9581,  8449,  1998, 16770,\n","         21025,  2705, 12083,  9006,  4502, 11129,  2243,  2818,  2527, 22083,\n","         29419,  9581,  6914,  8454, 23235,  2015,   102,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n"," 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n"," 'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["## Model Evaluation Methods\n","\n","\n","*   Precision: the ratio of correctly predicted positive observations to the total predicted positives.\n","*   Recall: the ratio of correctly predicted positive observations to the all observations in actual class.\n","*   accuracy: the ratio of correctly predicted observations to the total observations.\n","*   F1-score: the weighted average of precision and recall.\n","*   Hamming loss: the fraction of labels that are incorrectly predicted.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"jwQdYyju393N"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKrSFl7uAQSY"},"outputs":[],"source":["def multi_labels_metrics(predictions, labels, threshold=0.3):\n","    # creating an instance of the sigmoid activation function from the PyTorch library.\n","    sigmoid = torch.nn.Sigmoid()\n","    # convert the predicted scores (resulted from output layer of the model) into probabilities using the sigmoid function\n","    probs = sigmoid(torch.Tensor(predictions))\n","    # convert the probabilities into binary predictions based on the specified threshold.\n","    y_pred = np.zeros(probs.shape)\n","    y_pred[probs >= threshold] = 1\n","    y_true = labels\n","\n","\n","    # Calculate evaluation metrics\n","    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n","    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n","    Accuracy = accuracy_score(y_true, y_pred, normalize=False)\n","    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n","    hamming = hamming_loss(y_true, y_pred)\n","\n","    # average='weighted' -> Specifies the averaging method to compute precision.'weighted' takes the weighted average of the specefied metric (Precision, Recall, F1) across different classes, weighted by the number of true instances in each class.\n","    # zero_division=0 -> Specifies the action to take when a class with no true samples is encountered. Setting it to 0 ensures that if there are no true instances for a class, the metric for that class is considered as 0.\n","    # normalize=False -> Count of correctly classified samples.\n","    metrics = {\n","        \"hamming_loss\": hamming,\n","        \"f1\": f1,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"accuracy\": Accuracy,\n","    }\n","\n","    return metrics\n"]},{"cell_type":"code","source":["# Override compute_metrics method from transformers class (EvalPrediction)\n","def compute_metrics(p:EvalPrediction):\n","       preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions #to check if the predctions are stored in a tuple format, we are giong to take the first predction otherwayes it will select the precdtions directly\n","\n","       result = multi_labels_metrics(predictions=preds,labels=p.label_ids)\n","\n","       return result"],"metadata":{"id":"_3sKyn3GjhDL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training\n","training args:\n","\n","-per_device_train_batch_size -> Specifies the batch size of training data per device (such as GPU) during training.\n","\n","-per_device_eval_batch_size -> Specifies the batch size of evaluation data per device during evaluation.\n","\n","-output_dir -> Specifies the directory where the fine-tuned model and associated files will be saved.\n","\n","-num_train_epochs -> Specifies the number of epochs (complete passes through the training data) for training the model.\n","\n","-weight_decay -> Specifies the weight decay (L2 regularization) coefficient for controlling overfitting during training.\n","\n","-evaluation_strategy -> Specifies the strategy for evaluation during training. In this case, it's set to \"steps\", indicating that evaluation will be performed at fixed intervals defined by logging_steps.\n","\n","-logging_steps -> Specifies the frequency of logging training metrics and evaluation results during training.\n","\n","-lr_scheduler_type -> Specifies the type of learning rate scheduler used during training.\n","\n","-warmup_steps -> Specifies the number of warm-up steps for the learning rate scheduler.\n","\n","-learning_rate -> Sets the learning rate used by the optimizer during training. Here, the learning rate is set to 2e-05, which is a commonly used value for fine-tuning transformer-based models like DistilBERT for natural language processing tasks.\n","\n","-disable_tqdm -> Specifies whether to disable the progress bar (tqdm) during training and evaluation.\n","\n","-log_level -> Specifies the level of logging details. Here, it's set to \"error\", indicating that only error messages will be logged.\n","\n","-load_best_model_at_end -> Specifies whether to load the best model based on evaluation results at the end of training.\n","\n"],"metadata":{"id":"YIUPiJRdI0Zb"}},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer\n","\n","args = TrainingArguments(\n","    per_device_train_batch_size=20,\n","    per_device_eval_batch_size=20,\n","    output_dir = './finetunedDistilBERT',\n","    learning_rate=2e-05,\n","    seed=42,\n","    num_train_epochs=15,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"steps\",\n","    logging_steps=100,\n","    lr_scheduler_type=\"linear\",\n","    warmup_steps=500,\n","    disable_tqdm=False,\n","    log_level=\"error\",\n","    load_best_model_at_end=True\n",")\n","\n","trainer = Trainer(model=model,\n","                  args=args,\n","                  train_dataset = train_dataset,\n","                  eval_dataset = validat_dataset,\n","                  compute_metrics = compute_metrics)"],"metadata":{"id":"FkMLxlgSDOnk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LmQTXOmAAQMH","colab":{"base_uri":"https://localhost:8080/","height":141},"outputId":"da2b1dac-c6d5-4d74-f77f-3432dab4a26c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='165' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [165/165 02:32, Epoch 15/15]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Hamming Loss</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Accuracy</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","      <th>Steps Per Second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>0.076000</td>\n","      <td>0.085669</td>\n","      <td>0.033443</td>\n","      <td>0.581001</td>\n","      <td>0.498634</td>\n","      <td>0.707317</td>\n","      <td>4</td>\n","      <td>0.505800</td>\n","      <td>75.124000</td>\n","      <td>3.954000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=165, training_loss=0.0740394707882043, metrics={'train_runtime': 153.4258, 'train_samples_per_second': 33.436, 'train_steps_per_second': 1.075, 'total_flos': 398505015648000.0, 'train_loss': 0.0740394707882043, 'epoch': 15.0})"]},"metadata":{},"execution_count":47}],"source":["trainer.train()"]},{"cell_type":"markdown","source":["## Model Validation"],"metadata":{"id":"PsCP_tk1Ow9f"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNRjzdysAQJA","colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"1de27502-aaf6-4208-dfbc-afc1231a53a1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2/2 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.08402183651924133,\n"," 'eval_hamming_loss': 0.03289473684210526,\n"," 'eval_f1': 0.5863920696092281,\n"," 'eval_precision': 0.5082262367326434,\n"," 'eval_recall': 0.7073170731707317,\n"," 'eval_accuracy': 4,\n"," 'eval_runtime': 0.559,\n"," 'eval_samples_per_second': 67.982,\n"," 'eval_steps_per_second': 3.578,\n"," 'epoch': 15.0}"]},"metadata":{},"execution_count":48}],"source":["trainer.evaluate(validat_dataset)"]},{"cell_type":"markdown","source":["## Model Evaluation / Preformance Evaluation"],"metadata":{"id":"P3Gz3mQsC1Uj"}},{"cell_type":"code","source":["trainer.evaluate(test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"VPr-861lju3K","outputId":"fc0b008b-dc69-48b0-9f95-6ed394384d8b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2/2 00:06]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.06783310323953629,\n"," 'eval_hamming_loss': 0.027131782945736434,\n"," 'eval_f1': 0.6611561498269795,\n"," 'eval_precision': 0.5960357152307617,\n"," 'eval_recall': 0.7472527472527473,\n"," 'eval_accuracy': 9,\n"," 'eval_runtime': 0.7619,\n"," 'eval_samples_per_second': 56.439,\n"," 'eval_steps_per_second': 2.625,\n"," 'epoch': 15.0}"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["## Model Saving"],"metadata":{"id":"lrwuRCjPPAPA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"K392yKc7AQFu"},"outputs":[],"source":["# save the fine-tuned model\n","trainer.save_model(\"distilbert-finetuned-arxiv-multi-label\")"]},{"cell_type":"code","source":["# save the model tokenizer\n","tokenizer.save_pretrained(\"distilbert-finetuned-arxiv-multi-label/tokenizer\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XK_zE7xiAgXf","outputId":"b9b2699a-982e-43d9-f995-a03ac7ab8734"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('distilbert-finetuned-arxiv-multi-label/tokenizer/tokenizer_config.json',\n"," 'distilbert-finetuned-arxiv-multi-label/tokenizer/special_tokens_map.json',\n"," 'distilbert-finetuned-arxiv-multi-label/tokenizer/vocab.txt',\n"," 'distilbert-finetuned-arxiv-multi-label/tokenizer/added_tokens.json')"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2cBGd3UAQCc"},"outputs":[],"source":["# save the multilabelbinarizer object for later use at model inference\n","with open(\"multi-label-binarizer.pkl\", \"wb\") as f:\n","  pickle.dump(multilabel, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nezhJTbyAP_c","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b309c2da-5baf-4880-feb9-f2256a6a85bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/distilbert-finetuned-arxiv-multi-label/ (stored 0%)\n","  adding: content/distilbert-finetuned-arxiv-multi-label/model.safetensors (deflated 8%)\n","  adding: content/distilbert-finetuned-arxiv-multi-label/tokenizer/ (stored 0%)\n","  adding: content/distilbert-finetuned-arxiv-multi-label/tokenizer/tokenizer_config.json (deflated 74%)\n","  adding: content/distilbert-finetuned-arxiv-multi-label/tokenizer/special_tokens_map.json (deflated 42%)\n","  adding: content/distilbert-finetuned-arxiv-multi-label/tokenizer/vocab.txt (deflated 53%)\n","  adding: content/distilbert-finetuned-arxiv-multi-label/config.json (deflated 69%)\n","  adding: content/distilbert-finetuned-arxiv-multi-label/training_args.bin (deflated 51%)\n"]}],"source":["# save the model files as a zip file for easier download\n","!zip -r distilbert.zip \"/content/distilbert-finetuned-arxiv-multi-label\""]},{"cell_type":"markdown","metadata":{"id":"Y4v7i8sTUO5T"},"source":["## Model Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sm3zsUsZAP8T"},"outputs":[],"source":["text = \"A recently developed language representation model named Bidirectional Encoder Representation from Transformers (BERT) is based on an advanced trained deep learning approach that has achieved excellent results in many complex tasks, the same as classification, Natural Language Processing (NLP), prediction, etc. This survey paper mainly adopts the summary of BERT, its multiple types,  and  its  latest  developments  and  applications  in various  computer  science  and  engineering  fields. Furthermore,  it  puts  forward  BERT's  problems  and attractive future research trends in a different area with multiple datasets.  From the findings, overall, the BERT and their recent types have achieved more accurate, fast, and  optimal results  in  solving  most complex  problems than typical Machine and Deep Learning methods.\"\n","encoding = tokenizer(text, return_tensors='pt')\n","encoding.to(trainer.model.device)\n","\n","outputs = trainer.model(**encoding)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tj9jdpaGAP5u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"546108e5-294a-4682-ec64-de355ff8d81f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('cs.AI', 'cs.CL', 'cs.LG')]"]},"metadata":{},"execution_count":35}],"source":["# further output interpretation for clear representation of model prediction\n","sigmoid = torch.nn.Sigmoid()\n","probs = sigmoid(outputs.logits[0].cpu())\n","preds = np.zeros(probs.shape)\n","preds[np.where(probs>=0.3)] = 1\n","\n","multilabel.inverse_transform(preds.reshape(1,-1))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9ac6524909a64d7cb22552578aa6fd94":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_951d459f23834a14a05e2cd321399edc","IPY_MODEL_e6b509d39a5445658b5c3af3b4156e2b","IPY_MODEL_d20f232af3014e6c82f7a970fdc8caa4"],"layout":"IPY_MODEL_31dfff3148da4fa0a0cd6b31db510200"}},"951d459f23834a14a05e2cd321399edc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9777b7bca93c4b06a8e8ed0bd109000d","placeholder":"​","style":"IPY_MODEL_e61abb2e7acf49ed807f2ca1d21c3de9","value":"tokenizer_config.json: 100%"}},"e6b509d39a5445658b5c3af3b4156e2b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfc491b710f64acc83a7e9dbdbaa4ac9","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_abc52c469cec40f89d0134f92ed12e04","value":28}},"d20f232af3014e6c82f7a970fdc8caa4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49e1cfd4d7984e6e9aa4b7975d4c2e03","placeholder":"​","style":"IPY_MODEL_8959078bd6f3449b804919dba82ff8e5","value":" 28.0/28.0 [00:00&lt;00:00, 666B/s]"}},"31dfff3148da4fa0a0cd6b31db510200":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9777b7bca93c4b06a8e8ed0bd109000d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e61abb2e7acf49ed807f2ca1d21c3de9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfc491b710f64acc83a7e9dbdbaa4ac9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abc52c469cec40f89d0134f92ed12e04":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"49e1cfd4d7984e6e9aa4b7975d4c2e03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8959078bd6f3449b804919dba82ff8e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec158d9ca9d748d09c76e68d0abe6179":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41c9432ef4cf47d3ae04e08231c68c84","IPY_MODEL_2b8395ea76ef4d22bdbe6ed48671ebe8","IPY_MODEL_34cb45732762488b8ca90595c6af3615"],"layout":"IPY_MODEL_3ff6693d7bb24b94af62d35bb7af84a2"}},"41c9432ef4cf47d3ae04e08231c68c84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab2cc703b49740f191d518004cf89003","placeholder":"​","style":"IPY_MODEL_480cb67125924e80b2738702a427a41a","value":"vocab.txt: 100%"}},"2b8395ea76ef4d22bdbe6ed48671ebe8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_070db4772de247cf8a725e8039b1df53","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_272f329ccdc7423cb450f2adb1f21a46","value":231508}},"34cb45732762488b8ca90595c6af3615":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8736f9ecbe374d8097a09c9c5bf9526e","placeholder":"​","style":"IPY_MODEL_d1a2549ed0a94a1bab6bc158cf12f7e0","value":" 232k/232k [00:00&lt;00:00, 5.65MB/s]"}},"3ff6693d7bb24b94af62d35bb7af84a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab2cc703b49740f191d518004cf89003":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"480cb67125924e80b2738702a427a41a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"070db4772de247cf8a725e8039b1df53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"272f329ccdc7423cb450f2adb1f21a46":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8736f9ecbe374d8097a09c9c5bf9526e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1a2549ed0a94a1bab6bc158cf12f7e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d175e3587b3b419183477e7014d11ae8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1c76d47588144d08e06532234e8b025","IPY_MODEL_3532e35f4a8f44dcbf4ee1e9602c494e","IPY_MODEL_58555aea84bb4f5fa2008fc3012055c8"],"layout":"IPY_MODEL_f8ce01e2eb2c4b368883f977972a50c4"}},"e1c76d47588144d08e06532234e8b025":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c39f8000e5342d580d738c7a866e432","placeholder":"​","style":"IPY_MODEL_8ea2baa3f9d348c6b1795c629336a759","value":"tokenizer.json: 100%"}},"3532e35f4a8f44dcbf4ee1e9602c494e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_15f0fcba5d874545bcecf1d2976b56d8","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab589495861247dfa640159f77bc57e8","value":466062}},"58555aea84bb4f5fa2008fc3012055c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b70996ea360c45c98dede9e47bfc7242","placeholder":"​","style":"IPY_MODEL_048cbb15c92f4cbab36b9503e870f713","value":" 466k/466k [00:00&lt;00:00, 7.77MB/s]"}},"f8ce01e2eb2c4b368883f977972a50c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c39f8000e5342d580d738c7a866e432":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ea2baa3f9d348c6b1795c629336a759":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15f0fcba5d874545bcecf1d2976b56d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab589495861247dfa640159f77bc57e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b70996ea360c45c98dede9e47bfc7242":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"048cbb15c92f4cbab36b9503e870f713":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62e4c7b817e245b594a3abe1052ff473":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fbf1d870a44c4a9d904eaa638bb43723","IPY_MODEL_d1f03bbd25e3441b830a608ba7f8636a","IPY_MODEL_8eccd5e95f1942bdba8b095dde903193"],"layout":"IPY_MODEL_3acee30794e54226ab9535e0b6347fd1"}},"fbf1d870a44c4a9d904eaa638bb43723":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e96b6a1fb7f4159968550bbc65b31db","placeholder":"​","style":"IPY_MODEL_67bd25eb217c4643b7a4495569e9f94d","value":"config.json: 100%"}},"d1f03bbd25e3441b830a608ba7f8636a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5408283bb416486081b678b43f9cffa2","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d56be2960ee46c58f603cedc4bb5891","value":483}},"8eccd5e95f1942bdba8b095dde903193":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f153d844e51402f9e88c62b6e7013e7","placeholder":"​","style":"IPY_MODEL_13b68a3615e843c8965cdf550e3c0d17","value":" 483/483 [00:00&lt;00:00, 9.09kB/s]"}},"3acee30794e54226ab9535e0b6347fd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e96b6a1fb7f4159968550bbc65b31db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67bd25eb217c4643b7a4495569e9f94d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5408283bb416486081b678b43f9cffa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d56be2960ee46c58f603cedc4bb5891":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f153d844e51402f9e88c62b6e7013e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13b68a3615e843c8965cdf550e3c0d17":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1181a7edd984d2da5775e5c74011b6f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f07463f6388b469aa2efcc8ee6016ec9","IPY_MODEL_a90980e7cf014f28aad749fcdbbc8f51","IPY_MODEL_30281fb4ecee4c7189c8be2001dd8ddc"],"layout":"IPY_MODEL_21fd7e2838314f589b9589afa0d45cd8"}},"f07463f6388b469aa2efcc8ee6016ec9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19a558f2a86e407aacd968fe20cda968","placeholder":"​","style":"IPY_MODEL_721322ae1eef486b80a48c6e82550bca","value":"model.safetensors: 100%"}},"a90980e7cf014f28aad749fcdbbc8f51":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6521af96419148dcae08ba95d3738973","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26246fa0035b4dc59e20e27f4eb976c4","value":267954768}},"30281fb4ecee4c7189c8be2001dd8ddc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be5bd05ffaac4de0b13966131da6a1dd","placeholder":"​","style":"IPY_MODEL_b0afe099c0294e818abfc16ba7a38d58","value":" 268M/268M [00:01&lt;00:00, 181MB/s]"}},"21fd7e2838314f589b9589afa0d45cd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19a558f2a86e407aacd968fe20cda968":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"721322ae1eef486b80a48c6e82550bca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6521af96419148dcae08ba95d3738973":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26246fa0035b4dc59e20e27f4eb976c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be5bd05ffaac4de0b13966131da6a1dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0afe099c0294e818abfc16ba7a38d58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ee05d50dffb42958bc99478048823b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d2516ddea45451da9b69715f2ab5730","IPY_MODEL_58d7a98266684f48bec53363b8b0e247","IPY_MODEL_38f84c4b82cd47328ee449b4af501996"],"layout":"IPY_MODEL_d32167037f9b4509b210d66debe35509"}},"6d2516ddea45451da9b69715f2ab5730":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29671998bfab4c96bd1eede08b441e21","placeholder":"​","style":"IPY_MODEL_679f3918a8a3412b8caebbaef89d0889","value":"Downloading readme: 100%"}},"58d7a98266684f48bec53363b8b0e247":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bd065f5e7524f24925c8f1303f40cd7","max":267,"min":0,"orientation":"horizontal","style":"IPY_MODEL_313e28c96f5043bb9c310fcb98ce737e","value":267}},"38f84c4b82cd47328ee449b4af501996":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c8f5f8deb854017ada8efaa2dca6acc","placeholder":"​","style":"IPY_MODEL_3dfea1bfcfe64b7a869350089ab4c2d4","value":" 267/267 [00:00&lt;00:00, 17.2kB/s]"}},"d32167037f9b4509b210d66debe35509":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29671998bfab4c96bd1eede08b441e21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"679f3918a8a3412b8caebbaef89d0889":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8bd065f5e7524f24925c8f1303f40cd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"313e28c96f5043bb9c310fcb98ce737e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c8f5f8deb854017ada8efaa2dca6acc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dfea1bfcfe64b7a869350089ab4c2d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2a7710208b94b2d9e9b6ad5417a089b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_09adc0e6e7d9480d89eaa4d4cd356e7f","IPY_MODEL_622c3ac2ae4f4ef1b8ebed42c5b6690a","IPY_MODEL_9bbb985f0c8f4b47900a9d987777606e"],"layout":"IPY_MODEL_a330bbfbe3924eeb97e923c7022a68d5"}},"09adc0e6e7d9480d89eaa4d4cd356e7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b150f2ac63bd48b0b650a0a8b26930c6","placeholder":"​","style":"IPY_MODEL_976349b877fc4a188886148d8b63451b","value":"Downloading data: 100%"}},"622c3ac2ae4f4ef1b8ebed42c5b6690a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_205d16bef264488b9c737e35159d7181","max":38780102,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5bbfb18c211e4a4da59dbdd01dd5af99","value":38780102}},"9bbb985f0c8f4b47900a9d987777606e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_459b47e7fdd449d4a928296c411d97d4","placeholder":"​","style":"IPY_MODEL_0e8938d8f1b04aaab0fff3665adea805","value":" 38.8M/38.8M [00:01&lt;00:00, 37.8MB/s]"}},"a330bbfbe3924eeb97e923c7022a68d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b150f2ac63bd48b0b650a0a8b26930c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"976349b877fc4a188886148d8b63451b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"205d16bef264488b9c737e35159d7181":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bbfb18c211e4a4da59dbdd01dd5af99":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"459b47e7fdd449d4a928296c411d97d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e8938d8f1b04aaab0fff3665adea805":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"998c83e669f0424d8ff8e266078ee818":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8fcb10171be343e592e1f93501cce45f","IPY_MODEL_6e939acb564341be88eff10764481688","IPY_MODEL_42d015f6fa10487aa90ca29b49bac292"],"layout":"IPY_MODEL_01d6f917bee344309cad8c1dbde932e9"}},"8fcb10171be343e592e1f93501cce45f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a071a74c6c954d8bbc1c4f0c7c5906f0","placeholder":"​","style":"IPY_MODEL_c535c4b31d424d678d625284187bd416","value":"Generating train split: 100%"}},"6e939acb564341be88eff10764481688":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fc0feb049c04d99b32e5e4d791443af","max":423,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c27d009d39cd423ea6f876fd239712ec","value":423}},"42d015f6fa10487aa90ca29b49bac292":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef17f98f8c2c4e88a5fbc78587e946d8","placeholder":"​","style":"IPY_MODEL_e2e03eb9ecab42d59a8e52b747c42f45","value":" 423/423 [00:00&lt;00:00, 850.10 examples/s]"}},"01d6f917bee344309cad8c1dbde932e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a071a74c6c954d8bbc1c4f0c7c5906f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c535c4b31d424d678d625284187bd416":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3fc0feb049c04d99b32e5e4d791443af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c27d009d39cd423ea6f876fd239712ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef17f98f8c2c4e88a5fbc78587e946d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2e03eb9ecab42d59a8e52b747c42f45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}